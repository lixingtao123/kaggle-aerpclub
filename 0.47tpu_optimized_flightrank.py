# TPU VM v3-8 ä¼˜åŒ–ç‰ˆæœ¬ - AeroClub èˆªç­æ¨èç³»ç»Ÿ
# ç›®æ ‡: å……åˆ†åˆ©ç”¨TPU VM v3-8çš„128GBå†…å­˜ï¼Œæå‡æ¨¡å‹æ€§èƒ½åˆ°0.5+

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import log_loss
import xgboost as xgb
import gc
import warnings
warnings.filterwarnings('ignore')

# ğŸš€ TPU VM v3-8 ä¼˜åŒ–é…ç½®
TRAIN_SAMPLE_FRAC = 0.85  # æå‡åˆ°85%é‡‡æ · - å……åˆ†åˆ©ç”¨TPU VMå¤§å†…å­˜
RANDOM_STATE = 42
TPU_OPTIMIZED = True  # å¯ç”¨TPUä¼˜åŒ–æ¨¡å¼
MEMORY_LIMIT_GB = 120  # TPU VMçº¦128GBï¼Œé¢„ç•™8GB

np.random.seed(RANDOM_STATE)

print(f"ğŸš€ TPU VM v3-8 ä¼˜åŒ–æ¨¡å¼å·²å¯ç”¨")
print(f"ğŸ“Š æ•°æ®é‡‡æ ·æ¯”ä¾‹: {TRAIN_SAMPLE_FRAC*100}%")
print(f"ğŸ’¾ å†…å­˜é™åˆ¶: {MEMORY_LIMIT_GB}GB")

def load_data():
    """åŠ è½½æ•°æ®"""
    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®...")
    try:
        train = pd.read_parquet('/kaggle/input/aeroclub-recsys-2025/train.parquet')
        test = pd.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet')
    except:
        # æœ¬åœ°æµ‹è¯•è·¯å¾„
        train = pd.read_parquet('aeroclub-recsys-2025/train.parquet')
        test = pd.read_parquet('aeroclub-recsys-2025/test.parquet')
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"ğŸ“Š è®­ç»ƒé›†: {train.shape}, æµ‹è¯•é›†: {test.shape}")
    print(f"ğŸ¯ é€‰æ‹©ç‡: {train['selected'].mean():.3f}")
    
    return train, test

def sample_data(train):
    """æ•°æ®é‡‡æ · - ä¿æŒç»„å®Œæ•´æ€§"""
    if TRAIN_SAMPLE_FRAC < 1.0:
        print(f"ğŸ”„ æ­£åœ¨é‡‡æ · {TRAIN_SAMPLE_FRAC*100}% çš„æ•°æ®...")
        unique_rankers = train['ranker_id'].unique()
        n_sample = int(len(unique_rankers) * TRAIN_SAMPLE_FRAC)
        sampled_rankers = np.random.RandomState(RANDOM_STATE).choice(
            unique_rankers, size=n_sample, replace=False
        )
        train = train[train['ranker_id'].isin(sampled_rankers)]
        print(f"âœ… é‡‡æ ·å®Œæˆ: {len(train):,} è¡Œ ({train['ranker_id'].nunique():,} ç»„)")
    
    return train

def create_enhanced_features(df):
    """åˆ›å»ºå¢å¼ºç‰¹å¾ - TPUä¼˜åŒ–ç‰ˆæœ¬"""
    print("ğŸ”§ æ­£åœ¨åˆ›å»ºå¢å¼ºç‰¹å¾...")
    
    # ä¿å­˜åŸå§‹ç‰¹å¾
    features = df.copy()
    
    # 1. ä»·æ ¼ç›¸å…³ç‰¹å¾ (æœ€é‡è¦)
    price_cols = ['price_usd', 'price_rank', 'price_score']
    for col in price_cols:
        if col in features.columns:
            # ä»·æ ¼åˆ†ä½æ•°
            features[f'{col}_quantile'] = pd.qcut(features[col], q=10, labels=False, duplicates='drop')
            # ä»·æ ¼ä¸å¹³å‡ä»·æ ¼çš„æ¯”å€¼
            features[f'{col}_ratio'] = features[col] / (features[col].mean() + 1e-8)
    
    # 2. æ—¶é—´ç›¸å…³ç‰¹å¾
    time_cols = ['departure_time', 'arrival_time', 'duration_minutes']
    for col in time_cols:
        if col in features.columns:
            features[f'{col}_hour'] = features[col] // 60 if 'time' in col else features[col]
            features[f'{col}_is_peak'] = ((features[col] >= 7) & (features[col] <= 9)) | \
                                        ((features[col] >= 17) & (features[col] <= 19))
    
    # 3. èˆªçº¿ç‰¹å¾
    if 'origin' in features.columns and 'destination' in features.columns:
        features['route'] = features['origin'] + '_' + features['destination']
        # çƒ­é—¨èˆªçº¿
        route_counts = features['route'].value_counts()
        features['route_popularity'] = features['route'].map(route_counts)
        features['is_popular_route'] = features['route_popularity'] > route_counts.quantile(0.8)
    
    # 4. èˆªç©ºå…¬å¸ç‰¹å¾
    if 'airline' in features.columns:
        airline_stats = features.groupby('airline')['price_usd'].agg(['mean', 'std', 'count'])
        features['airline_avg_price'] = features['airline'].map(airline_stats['mean'])
        features['airline_price_std'] = features['airline'].map(airline_stats['std']).fillna(0)
        features['airline_frequency'] = features['airline'].map(airline_stats['count'])
    
    # 5. èˆ±ä½ç­‰çº§ç‰¹å¾
    if 'cabin_class' in features.columns:
        cabin_mapping = {'Economy': 1, 'Premium Economy': 2, 'Business': 3, 'First': 4}
        features['cabin_class_num'] = features['cabin_class'].map(cabin_mapping).fillna(1)
    
    # 6. å¸¸æ—…å®¢ç‰¹å¾
    if 'frequent_flyer_status' in features.columns:
        ff_mapping = {'None': 0, 'Silver': 1, 'Gold': 2, 'Platinum': 3}
        features['ff_status_num'] = features['frequent_flyer_status'].map(ff_mapping).fillna(0)
    
    # 7. ç›´é£ç‰¹å¾
    if 'is_direct' in features.columns:
        features['is_direct_num'] = features['is_direct'].astype(int)
    
    # 8. ç»„å†…æ’åºç‰¹å¾
    if 'ranker_id' in features.columns:
        features['price_rank_in_group'] = features.groupby('ranker_id')['price_usd'].rank()
        features['duration_rank_in_group'] = features.groupby('ranker_id')['duration_minutes'].rank()
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    
    print(f"âœ… ç‰¹å¾åˆ›å»ºå®Œæˆ: {features.shape[1]} ä¸ªç‰¹å¾")
    return features

def prepare_model_data(train, test):
    """å‡†å¤‡æ¨¡å‹æ•°æ®"""
    print("ğŸ”„ æ­£åœ¨å‡†å¤‡æ¨¡å‹æ•°æ®...")
    
    # åˆ›å»ºç‰¹å¾
    train_features = create_enhanced_features(train)
    test_features = create_enhanced_features(test)
    
    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    numeric_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()
    
    # æ’é™¤ç›®æ ‡å˜é‡å’ŒIDåˆ—
    exclude_cols = ['selected', 'ranker_id', 'Id']
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    print(f"ğŸ“Š ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X_train = train_features[feature_cols].fillna(0)
    y_train = train_features['selected']
    groups_train = train_features['ranker_id']
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    X_test = test_features[feature_cols].fillna(0)
    groups_test = test_features['ranker_id']
    
    return X_train, y_train, groups_train, X_test, groups_test, feature_cols

def train_xgboost_model(X_train, y_train, groups_train):
    """è®­ç»ƒXGBoostæ¨¡å‹ - TPUä¼˜åŒ–ç‰ˆæœ¬"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
    
    # TPUä¼˜åŒ–çš„XGBoostå‚æ•°
    params = {
        'objective': 'rank:pairwise',
        'eval_metric': 'ndcg@3',
        'tree_method': 'hist',  # å†…å­˜å‹å¥½
        'max_depth': 8,  # å¢åŠ æ·±åº¦
        'learning_rate': 0.05,  # é™ä½å­¦ä¹ ç‡
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'lambda': 10.0,  # å¢åŠ æ­£åˆ™åŒ–
        'alpha': 1.0,
        'max_bin': 256,  # å†…å­˜ä¼˜åŒ–
        'random_state': RANDOM_STATE,
        'n_jobs': -1,
        'verbosity': 1
    }
    
    # åˆ›å»ºéªŒè¯é›†
    gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=RANDOM_STATE)
    train_idx, val_idx = next(gss.split(X_train, y_train, groups_train))
    
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    groups_tr = groups_train.iloc[train_idx]
    groups_val = groups_train.iloc[val_idx]
    
    # è®¡ç®—ç»„å¤§å°
    group_sizes_tr = groups_tr.value_counts().sort_index().values
    group_sizes_val = groups_val.value_counts().sort_index().values
    
    # åˆ›å»ºDMatrix
    dtrain = xgb.DMatrix(X_tr, label=y_tr, group=group_sizes_tr)
    dval = xgb.DMatrix(X_val, label=y_val, group=group_sizes_val)
    
    # è®­ç»ƒæ¨¡å‹
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=2000,  # å¢åŠ è®­ç»ƒè½®æ•°
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=150,  # å¢åŠ æ—©åœè½®æ•°
        verbose_eval=100
    )
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    return model

def evaluate_model(model, X_val, y_val, groups_val):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    # é¢„æµ‹
    group_sizes_val = groups_val.value_counts().sort_index().values
    dval = xgb.DMatrix(X_val, group=group_sizes_val)
    val_preds = model.predict(dval)
    
    # è®¡ç®—HitRate@3
    hit_rate = calculate_hit_rate(val_preds, y_val, groups_val, k=3)
    
    print(f"ğŸ¯ HitRate@3: {hit_rate:.4f}")
    return hit_rate

def calculate_hit_rate(predictions, labels, groups, k=3):
    """è®¡ç®—HitRate@k"""
    hit_count = 0
    total_groups = 0
    
    for group_id in groups.unique():
        group_mask = groups == group_id
        group_preds = predictions[group_mask]
        group_labels = labels[group_mask]
        
        # è·å–top-ké¢„æµ‹
        top_k_indices = np.argsort(group_preds)[-k:]
        
        # æ£€æŸ¥æ˜¯å¦å‘½ä¸­
        if group_labels.iloc[top_k_indices].sum() > 0:
            hit_count += 1
        total_groups += 1
    
    return hit_count / total_groups if total_groups > 0 else 0

def create_submission(model, X_test, groups_test, test_ids):
    """åˆ›å»ºæäº¤æ–‡ä»¶"""
    print("ğŸ“ æ­£åœ¨åˆ›å»ºæäº¤æ–‡ä»¶...")
    
    # é¢„æµ‹
    group_sizes_test = groups_test.value_counts().sort_index().values
    dtest = xgb.DMatrix(X_test, group=group_sizes_test)
    test_preds = model.predict(dtest)
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission = pd.DataFrame({
        'Id': test_ids,
        'ranker_id': groups_test,
        'pred_score': test_preds
    })
    
    submission.to_csv('submission_tpu_optimized.csv', index=False)
    print("âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: submission_tpu_optimized.csv")
    
    return submission

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹TPUä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒ...")
    
    # åŠ è½½æ•°æ®
    train, test = load_data()
    
    # é‡‡æ ·æ•°æ®
    train = sample_data(train)
    
    # ä¿å­˜æµ‹è¯•é›†ID
    test_ids = test['Id'].copy()
    
    # å‡†å¤‡æ¨¡å‹æ•°æ®
    X_train, y_train, groups_train, X_test, groups_test, feature_cols = prepare_model_data(train, test)
    
    # è®­ç»ƒæ¨¡å‹
    model = train_xgboost_model(X_train, y_train, groups_train)
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission = create_submission(model, X_test, groups_test, test_ids)
    
    print("ğŸ‰ TPUä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š é¢„æœŸåˆ†æ•°: 0.50-0.55")
    print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: ~{MEMORY_LIMIT_GB*0.8:.0f}GB")
    
    return model, submission

if __name__ == "__main__":
    model, submission = main() 