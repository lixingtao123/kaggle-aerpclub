#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AeroClub RecSys 2025 - å¿«é€ŸXGBoostæ’åºæ¨¡å‹
åŸºäºé«˜æ•ˆnotebookæ”¹è¿›çš„ç”Ÿäº§çº§ä»£ç 

ä¸»è¦ç‰¹ç‚¹ï¼š
1. å¿«é€Ÿæ•°æ®é‡‡æ ·å’Œå¤„ç†
2. ç²¾ç®€ä½†æœ‰æ•ˆçš„ç‰¹å¾å·¥ç¨‹
3. é«˜æ•ˆçš„XGBoostè®­ç»ƒ
4. ç®€å•æ˜“ç”¨çš„æ¥å£
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import log_loss
import time
import gc
from typing import Tuple, Optional, Dict, Any
import warnings
warnings.filterwarnings('ignore')

class FastXGBoostRanker:
    """å¿«é€ŸXGBoostæ’åºæ¨¡å‹"""
    
    def __init__(self, 
                 sample_frac: float = 0.5,
                 random_state: int = 42,
                 n_jobs: int = -1):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            sample_frac: è®­ç»ƒæ•°æ®é‡‡æ ·æ¯”ä¾‹
            random_state: éšæœºç§å­
            n_jobs: å¹¶è¡Œä½œä¸šæ•°
        """
        self.sample_frac = sample_frac
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.model = None
        self.feature_cols = None
        self.cat_features = None
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(random_state)
        
        print(f"å¿«é€ŸXGBoostæ’åºæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"é‡‡æ ·æ¯”ä¾‹: {sample_frac}")
        print(f"éšæœºç§å­: {random_state}")
    
    def _get_categorical_features(self) -> list:
        """å®šä¹‰ç±»åˆ«ç‰¹å¾"""
        return [
            'nationality', 'searchRoute', 'corporateTariffCode',
            # Leg 0 segments 0-1
            'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',
            'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',
            'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',
            'legs0_segments0_flightNumber',
            'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',
            'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',
            'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',
            'legs0_segments1_flightNumber',
            # Leg 1 segments 0-1
            'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',
            'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',
            'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',
            'legs1_segments0_flightNumber',
            'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',
            'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',
            'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',
            'legs1_segments1_flightNumber'
        ]
    
    def _get_exclude_columns(self) -> list:
        """å®šä¹‰è¦æ’é™¤çš„åˆ—"""
        exclude_cols = [
            'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',
            'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',
            'miniRules0_percentage', 'miniRules1_percentage',  # >90% missing
            'frequentFlyer',  # å·²å¤„ç†
            'bySelf', 'pricingInfo_passengerCount',  # å¸¸é‡åˆ—
            # æ’é™¤baggageAllowance_weightMeasurementTypeåˆ—
            'legs0_segments0_baggageAllowance_weightMeasurementType',
            'legs0_segments1_baggageAllowance_weightMeasurementType',
            'legs1_segments0_baggageAllowance_weightMeasurementType',
            'legs1_segments1_baggageAllowance_weightMeasurementType',
        ]
        
        # æ’é™¤segment 2-3åˆ—ï¼ˆ>98%ç¼ºå¤±ï¼‰
        for leg in [0, 1]:
            for seg in [2, 3]:
                for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',
                              'baggageAllowance_quantity', 'baggageAllowance_weightMeasurementType',
                              'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',
                              'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:
                    exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')
        
        return exclude_cols
    
    def _hms_to_minutes(self, s: pd.Series) -> np.ndarray:
        """å°†'HH:MM:SS'æ ¼å¼è½¬æ¢ä¸ºåˆ†é’Ÿ"""
        mask = s.notna()
        out = np.zeros(len(s), dtype=float)
        if mask.any():
            parts = s[mask].astype(str).str.split(':', expand=True)
            out[mask] = (
                pd.to_numeric(parts[0], errors="coerce").fillna(0) * 60
                + pd.to_numeric(parts[1], errors="coerce").fillna(0)
            )
        return out
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºç‰¹å¾å·¥ç¨‹
        åŸºäºnotebookçš„é«˜æ•ˆç‰¹å¾å·¥ç¨‹æ–¹æ³•
        """
        print("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        start_time = time.time()
        
        df = df.copy()
        
        # å¤„ç†æ—¶é—´åˆ—
        dur_cols = (
            ["legs0_duration", "legs1_duration"]
            + [f"legs{l}_segments{s}_duration" for l in (0, 1) for s in (0, 1)]
        )
        for col in dur_cols:
            if col in df.columns:
                df[col] = self._hms_to_minutes(df[col])
        
        # ç‰¹å¾å®¹å™¨
        feat = {}
        
        # ä»·æ ¼ç‰¹å¾
        feat["price_per_tax"] = df["totalPrice"] / (df["taxes"] + 1)
        feat["tax_rate"] = df["taxes"] / (df["totalPrice"] + 1)
        feat["log_price"] = np.log1p(df["totalPrice"])
        
        # æ—¶é•¿ç‰¹å¾
        df["total_duration"] = df["legs0_duration"].fillna(0) + df["legs1_duration"].fillna(0)
        feat["duration_ratio"] = np.where(
            df["legs1_duration"].fillna(0) > 0,
            df["legs0_duration"] / (df["legs1_duration"] + 1),
            1.0,
        )
        
        # èˆªæ®µæ•°é‡ç‰¹å¾
        for leg in (0, 1):
            seg_count = 0
            for seg in range(4):
                col = f"legs{leg}_segments{seg}_duration"
                if col in df.columns:
                    seg_count += df[col].notna().astype(int)
                else:
                    break
            feat[f"n_segments_leg{leg}"] = seg_count
        
        feat["total_segments"] = feat["n_segments_leg0"] + feat["n_segments_leg1"]
        
        # è¡Œç¨‹ç±»å‹æ£€æµ‹
        feat["is_one_way"] = (
            df["legs1_duration"].isna() | 
            (df["legs1_duration"] == 0) |
            df["legs1_segments0_departureFrom_airport_iata"].isna()
        ).astype(int)
        feat["has_return"] = (1 - feat["is_one_way"]).astype(int)
        
        # æ’åç‰¹å¾
        grp = df.groupby("ranker_id")
        feat["price_rank"] = grp["totalPrice"].rank()
        feat["price_pct_rank"] = grp["totalPrice"].rank(pct=True)
        feat["duration_rank"] = grp["total_duration"].rank()
        feat["is_cheapest"] = (grp["totalPrice"].transform("min") == df["totalPrice"]).astype(int)
        feat["is_most_expensive"] = (grp["totalPrice"].transform("max") == df["totalPrice"]).astype(int)
        feat["price_from_median"] = grp["totalPrice"].transform(
            lambda x: (x - x.median()) / (x.std() + 1)
        )
        
        # å¸¸æ—…å®¢ç‰¹å¾
        ff = df["frequentFlyer"].fillna("").astype(str)
        feat["n_ff_programs"] = ff.str.count("/") + (ff != "")
        
        # ä¸»è¦èˆªç©ºå…¬å¸çš„å¸¸æ—…å®¢è®¡åˆ’
        for al in ["SU", "S7", "U6", "TK"]:
            feat[f"ff_{al}"] = ff.str.contains(rf"\b{al}\b").astype(int)
        
        # å¸¸æ—…å®¢ä¸æ‰¿è¿äººåŒ¹é…
        feat["ff_matches_carrier"] = 0
        for al in ["SU", "S7", "U6", "TK"]:
            if "legs0_segments0_marketingCarrier_code" in df.columns:
                feat["ff_matches_carrier"] |= (
                    (feat.get(f"ff_{al}", 0) == 1) & 
                    (df["legs0_segments0_marketingCarrier_code"] == al)
                ).astype(int)
        
        # äºŒå…ƒç‰¹å¾
        feat["is_vip_freq"] = ((df["isVip"] == 1) | (feat["n_ff_programs"] > 0)).astype(int)
        feat["has_corporate_tariff"] = (~df["corporateTariffCode"].isna()).astype(int)
        
        # è¡Œæå’Œè´¹ç”¨ç‰¹å¾
        feat["baggage_total"] = (
            df["legs0_segments0_baggageAllowance_quantity"].fillna(0)
            + df["legs1_segments0_baggageAllowance_quantity"].fillna(0)
        )
        feat["has_baggage"] = (feat["baggage_total"] > 0).astype(int)
        feat["total_fees"] = (
            df["miniRules0_monetaryAmount"].fillna(0) + df["miniRules1_monetaryAmount"].fillna(0)
        )
        feat["has_fees"] = (feat["total_fees"] > 0).astype(int)
        feat["fee_rate"] = feat["total_fees"] / (df["totalPrice"] + 1)
        
        # æ—¶é—´ç‰¹å¾
        for col in ("legs0_departureAt", "legs0_arrivalAt", "legs1_departureAt", "legs1_arrivalAt"):
            if col in df.columns:
                dt = pd.to_datetime(df[col], errors="coerce")
                feat[f"{col}_hour"] = dt.dt.hour.fillna(12)
                feat[f"{col}_weekday"] = dt.dt.weekday.fillna(0)
                h = dt.dt.hour.fillna(12)
                feat[f"{col}_business_time"] = (((6 <= h) & (h <= 9)) | ((17 <= h) & (h <= 20))).astype(int)
        
        # ç›´é£ç‰¹å¾
        feat["is_direct_leg0"] = (feat["n_segments_leg0"] == 1).astype(int)
        feat["is_direct_leg1"] = np.where(
            feat["is_one_way"] == 1,
            0,
            (feat["n_segments_leg1"] == 1).astype(int)
        )
        feat["both_direct"] = (feat["is_direct_leg0"] & feat["is_direct_leg1"]).astype(int)
        
        # æœ€ä¾¿å®œçš„ç›´é£
        df["_is_direct"] = feat["is_direct_leg0"] == 1
        direct_groups = df[df["_is_direct"]].groupby("ranker_id")["totalPrice"]
        if len(direct_groups) > 0:
            direct_min_price = direct_groups.min()
            feat["is_direct_cheapest"] = (
                df["_is_direct"] & 
                (df["totalPrice"] == df["ranker_id"].map(direct_min_price))
            ).astype(int)
        else:
            feat["is_direct_cheapest"] = 0
        df.drop(columns="_is_direct", inplace=True)
        
        # å…¶ä»–ç‰¹å¾
        feat["has_access_tp"] = (df["pricingInfo_isAccessTP"] == 1).astype(int)
        feat["group_size"] = df.groupby("ranker_id")["Id"].transform("count")
        feat["group_size_log"] = np.log1p(feat["group_size"])
        
        # ä¸»è¦æ‰¿è¿äºº
        if "legs0_segments0_marketingCarrier_code" in df.columns:
            feat["is_major_carrier"] = df["legs0_segments0_marketingCarrier_code"].isin(["SU", "S7", "U6"]).astype(int)
        else:
            feat["is_major_carrier"] = 0
        
        # çƒ­é—¨è·¯çº¿
        popular_routes = {"MOWLED/LEDMOW", "LEDMOW/MOWLED", "MOWLED", "LEDMOW", "MOWAER/AERMOW"}
        feat["is_popular_route"] = df["searchRoute"].isin(popular_routes).astype(int)
        
        # èˆ±ä½ç­‰çº§ç‰¹å¾
        feat["avg_cabin_class"] = df[["legs0_segments0_cabinClass", "legs1_segments0_cabinClass"]].mean(axis=1)
        feat["cabin_class_diff"] = (
            df["legs0_segments0_cabinClass"].fillna(0) - df["legs1_segments0_cabinClass"].fillna(0)
        )
        
        # åˆå¹¶æ–°ç‰¹å¾
        df = pd.concat([df, pd.DataFrame(feat, index=df.index)], axis=1)
        
        # å¡«å……ç¼ºå¤±å€¼
        for col in df.select_dtypes(include="number").columns:
            df[col] = df[col].fillna(0)
        for col in df.select_dtypes(include="object").columns:
            df[col] = df[col].fillna("missing")
        
        elapsed_time = time.time() - start_time
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        print(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {df.shape[1]}")
        
        return df
    
    def load_and_sample_data(self, train_path:str, test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åŠ è½½å’Œé‡‡æ ·æ•°æ®"""
        print("åŠ è½½æ•°æ®...")
        start_time = time.time()
        
        # åŠ è½½æ•°æ®
        train = pd.read_parquet(train_path)
        test = pd.read_parquet(test_path)
        
        print(f"åŸå§‹æ•°æ®: Train {train.shape}, Test {test.shape}")
        print(f"å”¯ä¸€ranker_ids: {train['ranker_id'].nunique():,}")
        print(f"é€‰æ‹©ç‡: {train['selected'].mean():.3f}")
        
        # æŒ‰ranker_idé‡‡æ ·ä»¥ä¿æŒç»„å®Œæ•´æ€§
        if self.sample_frac < 1.0:
            unique_rankers = train['ranker_id'].unique()
            n_sample = int(len(unique_rankers) * self.sample_frac)
            sampled_rankers = np.random.RandomState(self.random_state).choice(
                unique_rankers, size=n_sample, replace=False
            )
            train = train[train['ranker_id'].isin(sampled_rankers)]
            print(f"é‡‡æ ·åæ•°æ®: {len(train):,} rows ({train['ranker_id'].nunique():,} groups)")
        
        # è½¬æ¢ranker_idä¸ºå­—ç¬¦ä¸²
        train['ranker_id'] = train['ranker_id'].astype(str)
        test['ranker_id'] = test['ranker_id'].astype(str)
        
        elapsed_time = time.time() - start_time
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return train, test
    
    def prepare_features(self, train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, list]:
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        print("å‡†å¤‡ç‰¹å¾æ•°æ®...")
        
        # åº”ç”¨ç‰¹å¾å·¥ç¨‹
        train = self.create_features(train)
        test = self.create_features(test)
        
        # è·å–ç‰¹å¾åˆ—
        cat_features = self._get_categorical_features()
        exclude_cols = self._get_exclude_columns()
        
        feature_cols = [col for col in train.columns if col not in exclude_cols]
        cat_features_final = [col for col in cat_features if col in feature_cols]
        
        print(f"ä½¿ç”¨ {len(feature_cols)} ä¸ªç‰¹å¾ ({len(cat_features_final)} ä¸ªç±»åˆ«ç‰¹å¾)")
        
        # å­˜å‚¨ç‰¹å¾ä¿¡æ¯
        self.feature_cols = feature_cols
        self.cat_features = cat_features_final
        
        return train, test, feature_cols
    
    def encode_categorical_features(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame):
        """ç¼–ç ç±»åˆ«ç‰¹å¾"""
        print("ç¼–ç ç±»åˆ«ç‰¹å¾...")
        
        X_train_encoded = X_train.copy()
        X_val_encoded = X_val.copy()
        X_test_encoded = X_test.copy()
        
        for col in self.cat_features:
            if col in X_train_encoded.columns:
                # åˆ›å»ºæ˜ å°„
                unique_vals = pd.concat([X_train_encoded[col], X_val_encoded[col], X_test_encoded[col]]).unique()
                mapping = {val: idx for idx, val in enumerate(unique_vals)}
                
                X_train_encoded[col] = X_train_encoded[col].map(mapping).fillna(-1).astype(int)
                X_val_encoded[col] = X_val_encoded[col].map(mapping).fillna(-1).astype(int)
                X_test_encoded[col] = X_test_encoded[col].map(mapping).fillna(-1).astype(int)
        
        return X_train_encoded, X_val_encoded, X_test_encoded
    
    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, 
                   X_val: pd.DataFrame, y_val: pd.Series,
                   groups_train: pd.Series, groups_val: pd.Series) -> xgb.Booster:
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        print("è®­ç»ƒXGBoostæ¨¡å‹...")
        start_time = time.time()
        
        # ç¼–ç ç±»åˆ«ç‰¹å¾
        X_train_encoded, X_val_encoded, _ = self.encode_categorical_features(X_train, X_val, X_train)
        
        # åˆ›å»ºç»„å¤§å°
        group_sizes_train = pd.DataFrame(groups_train).groupby('ranker_id').size().values
        group_sizes_val = pd.DataFrame(groups_val).groupby('ranker_id').size().values
        
        # åˆ›å»ºDMatrix
        dtrain = xgb.DMatrix(X_train_encoded, label=y_train, group=group_sizes_train)
        dval = xgb.DMatrix(X_val_encoded, label=y_val, group=group_sizes_val)
        
        # XGBoostå‚æ•°
        params = {
            'objective': 'rank:pairwise',
            'eval_metric': 'ndcg@3',
            'max_depth': 8,
            'min_child_weight': 10,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'lambda': 10.0,
            'learning_rate': 0.05,
            'seed': self.random_state,
            'n_jobs': self.n_jobs,
            'verbosity': 1
        }
        
        # è®­ç»ƒæ¨¡å‹
        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=1500,
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=100,
            verbose_eval=50
        )
        
        elapsed_time = time.time() - start_time
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return self.model
    
    def evaluate_model(self, y_true: pd.Series, y_pred: np.ndarray, groups: pd.Series, model_name: str = "Model") -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        def sigmoid(x):
            return 1 / (1 + np.exp(-x / 10))
        
        def calculate_hitrate_at_k(df, k=3):
            """è®¡ç®—HitRate@k"""
            hits = []
            for ranker_id, group in df.groupby('ranker_id'):
                if len(group) > 10:
                    top_k = group.nlargest(k, 'pred')
                    hit = (top_k['selected'] == 1).any()
                    hits.append(hit)
            return np.mean(hits) if hits else 0.0
        
        df = pd.DataFrame({
            'ranker_id': groups,
            'pred': y_pred,
            'selected': y_true
        })
        
        # æ¯ç»„çš„topé¢„æµ‹
        top_preds = df.loc[df.groupby('ranker_id')['pred'].idxmax()]
        top_preds['prob'] = sigmoid(top_preds['pred'])
        
        # è®¡ç®—æŒ‡æ ‡
        logloss = log_loss(top_preds['selected'], top_preds['prob'])
        hitrate_at_3 = calculate_hitrate_at_k(df, k=3)
        accuracy = (top_preds['selected'] == 1).mean()
        
        metrics = {
            'hitrate_at_3': hitrate_at_3,
            'logloss': logloss,
            'accuracy': accuracy
        }
        
        print(f"{model_name} éªŒè¯æŒ‡æ ‡:")
        print(f"HitRate@3 (groups >10): {hitrate_at_3:.4f}")
        print(f"LogLoss:                {logloss:.4f}")
        print(f"Top-1 Accuracy:         {accuracy:.4f}")
        
        return metrics
    
    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•è·å–ç‰¹å¾é‡è¦æ€§")
        
        importance = self.model.get_score(importance_type='gain')
        importance_df = pd.DataFrame([
            {'feature': k, 'importance': v} 
            for k, v in importance.items()
        ]).sort_values('importance', ascending=False)
        
        print(f"Top {top_n} é‡è¦ç‰¹å¾:")
        print(importance_df.head(top_n).to_string(index=False))
        
        return importance_df
    
    def predict(self, X_test: pd.DataFrame, groups_test: pd.Series) -> pd.DataFrame:
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹")
        
        print("ç”Ÿæˆé¢„æµ‹ç»“æœ...")
        start_time = time.time()
        
        # ç¼–ç ç±»åˆ«ç‰¹å¾
        _, _, X_test_encoded = self.encode_categorical_features(X_test, X_test, X_test)
        
        # åˆ›å»ºDMatrix
        group_sizes_test = pd.DataFrame(groups_test).groupby('ranker_id').size().values
        dtest = xgb.DMatrix(X_test_encoded, group=group_sizes_test)
        
        # é¢„æµ‹
        test_preds = self.model.predict(dtest)
        
        elapsed_time = time.time() - start_time
        print(f"é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return test_preds
    
    def run_full_pipeline(self, train_path: str, test_path: str, output_path: str = 'submission.csv') -> pd.DataFrame:
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("=== å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹ ===")
        total_start_time = time.time()
        
        # 1. åŠ è½½å’Œé‡‡æ ·æ•°æ®
        train, test = self.load_and_sample_data(train_path, test_path)
        
        # 2. ç‰¹å¾å·¥ç¨‹
        train, test, feature_cols = self.prepare_features(train, test)
        
        # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = train[feature_cols]
        y_train = train['selected']
        groups_train = train['ranker_id']
        
        X_test = test[feature_cols]
        groups_test = test['ranker_id']
        
        # 4. åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=self.random_state)
        train_idx, val_idx = next(gss.split(X_train, y_train, groups_train))
        
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        groups_tr, groups_val = groups_train.iloc[train_idx], groups_train.iloc[val_idx]
        
        print(f"æ•°æ®åˆ†å‰²: Train {len(X_tr):,}, Val {len(X_val):,}, Test {len(X_test):,}")
        
        # 5. ä¿®å¤æ•°æ®ç±»å‹
        X_tr, X_val, X_test = self.fix_data_types(X_tr, X_val, X_test)
        
        # 6. è®¡ç®—ç»„å¤§å°
        group_sizes_tr = self.get_group_sizes(groups_tr)
        group_sizes_val = self.get_group_sizes(groups_val)
        
        print(f"   è®­ç»ƒç»„æ•°: {len(group_sizes_tr)}, éªŒè¯ç»„æ•°: {len(group_sizes_val)}")
        
        # 7. åˆ›å»ºDMatrix
        print("ğŸš€ åˆ›å»ºDMatrix...")
        try:
            # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ•°æ®ç±»å‹
            print(f"   X_træ•°æ®ç±»å‹: {X_tr.dtypes.value_counts().to_dict()}")
            print(f"   æ˜¯å¦æœ‰objectç±»å‹: {(X_tr.dtypes == 'object').any()}")
            
            dtrain = xgb.DMatrix(X_tr, label=y_tr, group=group_sizes_tr)
            dval = xgb.DMatrix(X_val, label=y_val, group=group_sizes_val)
            print("   âœ… DMatrixåˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"   âŒ DMatrixåˆ›å»ºå¤±è´¥: {e}")
            print("   ğŸ”„ å°è¯•å¯ç”¨åˆ†ç±»ç‰¹å¾æ”¯æŒ...")
            
            try:
                dtrain = xgb.DMatrix(X_tr, label=y_tr, group=group_sizes_tr, enable_categorical=True)
                dval = xgb.DMatrix(X_val, label=y_val, group=group_sizes_val, enable_categorical=True)
                print("   âœ… å¯ç”¨åˆ†ç±»ç‰¹å¾çš„DMatrixåˆ›å»ºæˆåŠŸ")
            except Exception as e2:
                print(f"   âŒ å¯ç”¨åˆ†ç±»ç‰¹å¾ä¹Ÿå¤±è´¥: {e2}")
                print("   ğŸ”„ å°è¯•çº¯æ•°å€¼æ–¹æ³•...")
                
                try:
                    # ğŸ”¥ åªä¿ç•™æ•°å€¼åˆ—
                    numeric_cols = X_tr.select_dtypes(include=[np.number]).columns
                    print(f"   ä½¿ç”¨ {len(numeric_cols)} ä¸ªæ•°å€¼åˆ—")
                    
                    X_tr_numeric = X_tr[numeric_cols].fillna(0).astype(np.float32)
                    X_val_numeric = X_val[numeric_cols].fillna(0).astype(np.float32)
                    
                    dtrain = xgb.DMatrix(X_tr_numeric, label=y_tr, group=group_sizes_tr)
                    dval = xgb.DMatrix(X_val_numeric, label=y_val, group=group_sizes_val)
                    print("   âœ… æ•°å€¼åˆ—DMatrixåˆ›å»ºæˆåŠŸ")
                    
                    # ğŸ”¥ æ›´æ–°X_testä»¥ä¿æŒä¸€è‡´
                    X_test = X_test[numeric_cols].fillna(0).astype(np.float32)
                    
                except Exception as e3:
                    print(f"   âŒ æ•°å€¼åˆ—æ–¹æ³•ä¹Ÿå¤±è´¥: {e3}")
                    print("   ğŸ”„ æœ€åå°è¯•ï¼šç®€åŒ–åˆ°äºŒåˆ†ç±»...")
                    
                    try:
                        # ğŸ”¥ æœ€åæ–¹æ¡ˆï¼šæ”¾å¼ƒrankingï¼Œåªç”¨äºŒåˆ†ç±»
                        X_tr_simple = X_tr.select_dtypes(include=[np.number]).fillna(0).values.astype(np.float32)
                        X_val_simple = X_val.select_dtypes(include=[np.number]).fillna(0).values.astype(np.float32)
                        
                        dtrain = xgb.DMatrix(X_tr_simple, label=y_tr.values)
                        dval = xgb.DMatrix(X_val_simple, label=y_val.values)
                        print("   âœ… ç®€åŒ–äºŒåˆ†ç±»DMatrixåˆ›å»ºæˆåŠŸ")
                        
                        # ğŸ”¥ æ›´æ–°X_test
                        X_test = pd.DataFrame(X_test.select_dtypes(include=[np.number]).fillna(0).values.astype(np.float32))
                        
                    except Exception as e4:
                        print(f"   ğŸ’¥ æ‰€æœ‰DMatrixåˆ›å»ºæ–¹æ³•éƒ½å¤±è´¥äº†: {e4}")
                        raise RuntimeError("æ— æ³•åˆ›å»ºXGBoost DMatrixï¼Œè¯·æ£€æŸ¥æ•°æ®")
        
        # 8. é…ç½®XGBoostå‚æ•°
        print("âš™ï¸ é…ç½®XGBoostå‚æ•°...")
        xgb_params = {
            'objective': 'binary:logistic',  # ä½¿ç”¨äºŒåˆ†ç±»ï¼Œæ›´ç¨³å®š
            'eval_metric': 'logloss',
            'max_depth': 6,                  # é™ä½å¤æ‚åº¦
            'min_child_weight': 5,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'lambda': 1.0,
            'learning_rate': 0.1,
            'seed': 42,
            'tree_method': 'hist',           # ä½¿ç”¨histogramæ–¹æ³•ï¼Œæ›´å¿«æ›´ç¨³å®š
            'n_jobs': -1,
            'verbosity': 1
        }
        
        # 9. è®­ç»ƒæ¨¡å‹
        print("ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒXGBoost...")
        try:
            xgb_model = xgb.train(
                xgb_params,
                dtrain,
                num_boost_round=100,  # å‡å°‘è½®æ•°ï¼Œæ›´å¿«
                evals=[(dtrain, 'train'), (dval, 'val')],
                early_stopping_rounds=20,
                verbose_eval=10
            )
            print("   âœ… XGBoostè®­ç»ƒå®Œæˆ")
        except Exception as e:
            print(f"   âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise
        
        # 10. æ¸…ç†å†…å­˜
        print("ğŸ§¹ æ¸…ç†è®­ç»ƒæ•°æ®...")
        del dtrain, X_tr, y_tr, groups_tr
        gc.collect()
        print("   âœ… è®­ç»ƒæ•°æ®æ¸…ç†å®Œæˆ")
        
        # 11. éªŒè¯è¯„ä¼°
        _, X_val_encoded, _ = self.encode_categorical_features(X_tr, X_val, X_test)
        group_sizes_val = pd.DataFrame(groups_val).groupby('ranker_id').size().values
        dval = xgb.DMatrix(X_val_encoded, group=group_sizes_val)
        val_preds = xgb_model.predict(dval)
        
        metrics = self.evaluate_model(y_val, val_preds, groups_val, "XGBoost")
        
        # 12. ç‰¹å¾é‡è¦æ€§
        self.get_feature_importance()
        
        # 13. ç”Ÿæˆæµ‹è¯•é¢„æµ‹
        test_preds = self.predict(X_test, groups_test)
        
        # 14. åˆ›å»ºæäº¤æ–‡ä»¶
        submission = test[['Id', 'ranker_id']].copy()
        submission['pred_score'] = test_preds
        submission['selected'] = submission.groupby('ranker_id')['pred_score'].rank(
            ascending=False, method='first'
        ).astype(int)
        
        # ä¿å­˜ç»“æœ
        output_file = 'ultra_fast_submission.csv'
        final_submission = submission[['Id', 'ranker_id', 'selected']]
        final_submission.to_csv(output_file, index=False)
        
        total_elapsed_time = time.time() - total_start_time
        print(f"\n=== å®Œæ•´æµç¨‹å®Œæˆ ===")
        print(f"æ€»è€—æ—¶: {total_elapsed_time:.2f}ç§’")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ç”Ÿæˆäº† {len(final_submission):,} è¡Œé¢„æµ‹ç»“æœ")
        
        return final_submission


def quick_run(train_path: str, test_path: str, output_path: str = 'submission.csv', 
              sample_frac: float = 0.5, random_state: int = 42) -> pd.DataFrame:
    """
    å¿«é€Ÿè¿è¡Œå‡½æ•°
    
    Args:
        train_path: è®­ç»ƒæ•°æ®è·¯å¾„
        test_path: æµ‹è¯•æ•°æ®è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        sample_frac: é‡‡æ ·æ¯”ä¾‹
        random_state: éšæœºç§å­
        
    Returns:
        pd.DataFrame: æäº¤ç»“æœ
    """
    print("=== å¿«é€ŸXGBoostæ¨¡å‹ ===")
    print(f"æ–‡ä»¶é…ç½®:")
    print(f"   è®­ç»ƒæ•°æ®: {train_path}")
    print(f"   æµ‹è¯•æ•°æ®: {test_path}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {sample_frac}")
    print()
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = FastXGBoostRanker(
            sample_frac=sample_frac,
            random_state=random_state
        )
        
        # è¿è¡Œæµç¨‹
        submission = model.run_full_pipeline(train_path, test_path, output_path)
        
        print("è¿è¡Œå®Œæˆï¼")
        return submission
        
    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()


# ä¸»å‡½æ•°ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    TRAIN_FILE_PATH = '/kaggle/input/aeroclub-recsys-2025/train.parquet'
    TEST_FILE_PATH = '/kaggle/input/aeroclub-recsys-2025/test.parquet'
    OUTPUT_FILE_PATH = 'fast_submission.csv'
    SAMPLE_FRAC = 0.5  # é‡‡æ ·æ¯”ä¾‹
    RANDOM_STATE = 42
    
    print("=== AeroClub RecSys 2025 - å¿«é€ŸXGBoostæ’åºæ¨¡å‹ ===")
    print(f"åŸºäºé«˜æ•ˆnotebookæ”¹è¿›çš„ç”Ÿäº§çº§ä»£ç ")
    print()
    
    # å¿«é€Ÿè¿è¡Œ
    submission = quick_run(
        TRAIN_FILE_PATH,
        TEST_FILE_PATH,
        OUTPUT_FILE_PATH,
        sample_frac=SAMPLE_FRAC,
        random_state=RANDOM_STATE
    )
    
    if not submission.empty:
        print(f"\nä»»åŠ¡å®Œæˆï¼ç”Ÿæˆäº† {len(submission)} è¡Œé¢„æµ‹ç»“æœ")
        print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE_PATH}")
    else:
        print("\nä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œé”™è¯¯ä¿¡æ¯")

# ========== æç®€ç‰¹å¾å·¥ç¨‹ ==========
print("ğŸš€ æç®€ç‰¹å¾å·¥ç¨‹")

def minimal_features(df):
    """æœ€å°‘ä½†æœ‰æ•ˆçš„ç‰¹å¾ - è¶…å¿«ç‰ˆæœ¬"""
    
    print(f"   å¤„ç†æ•°æ®: {df.shape}")
    
    # ğŸ”¥ è·³è¿‡å¤æ‚çš„æ—¶é—´å¤„ç†ï¼Œç›´æ¥ç”¨ç®€å•ç‰¹å¾
    # ä¸å¤„ç†durationåˆ—ï¼Œå› ä¸ºå¤ªæ…¢äº†
    
    # æœ€åŸºç¡€çš„ä»·æ ¼ç‰¹å¾
    df['log_price'] = np.log1p(df['totalPrice']).astype(np.float32)
    
    # ğŸ”¥ å¿«é€Ÿåˆ¤æ–­æ˜¯å¦å•ç¨‹ - åŸºäºåˆ—æ˜¯å¦å­˜åœ¨å€¼
    if 'legs1_duration' in df.columns:
        df['is_one_way'] = df['legs1_duration'].isna().astype(np.int8)
    else:
        df['is_one_way'] = 1  # å¦‚æœæ²¡æœ‰leg1åˆ—ï¼Œå°±æ˜¯å•ç¨‹
    
    # ğŸ”¥ æœ€é‡è¦ï¼šç»„å†…ä»·æ ¼æ’å
    print("   è®¡ç®—ä»·æ ¼æ’å...")
    df['price_rank'] = df.groupby('ranker_id')['totalPrice'].rank(method='dense').astype(np.int16)
    
    print("   è®¡ç®—æœ€ä¾¿å®œæ ‡è®°...")
    df['is_cheapest'] = (df.groupby('ranker_id')['totalPrice'].transform('min') == 
                        df['totalPrice']).astype(np.int8)
    
    # ğŸ”¥ ç®€å•çš„ç¨è´¹æ¯”ç‡
    df['tax_ratio'] = (df['taxes'] / (df['totalPrice'] + 1)).astype(np.float32)
    
    print("   ç‰¹å¾å·¥ç¨‹å®Œæˆ")
    return df

# åº”ç”¨ç‰¹å¾å·¥ç¨‹
print("ğŸ”§ è®­ç»ƒæ•°æ®ç‰¹å¾å·¥ç¨‹...")
train = minimal_features(train)

print("ğŸ”§ æµ‹è¯•æ•°æ®ç‰¹å¾å·¥ç¨‹...")  
test = minimal_features(test)

print(f"âœ… æç®€ç‰¹å¾å·¥ç¨‹å®Œæˆ")
print(f"   è®­ç»ƒæ•°æ®: {train.shape}")
print(f"   æµ‹è¯•æ•°æ®: {test.shape}")

# ç«‹å³æ¸…ç†å†…å­˜
import gc
gc.collect()

# ========== ä¿®å¤æ•°æ®ç±»å‹å¹¶è®­ç»ƒXGBoost ==========
print("ğŸš€ ä¿®å¤æ•°æ®ç±»å‹å¹¶è®­ç»ƒXGBoost")

# ğŸ”¥ é¦–å…ˆä¿®å¤æ‰€æœ‰æ•°æ®ç±»å‹é—®é¢˜
def fix_data_types(X_tr, X_val, X_test):
    """ä¿®å¤æ‰€æœ‰æ•°æ®ç±»å‹é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤æ•°æ®ç±»å‹...")
    
    # ğŸ”¥ åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
    X_tr = X_tr.copy()
    X_val = X_val.copy() 
    X_test = X_test.copy()
    
    # å¤„ç†durationåˆ— - å¦‚æœæ˜¯objectï¼Œè½¬æ¢ä¸º0
    for col in ['legs0_duration', 'legs1_duration']:
        if col in X_tr.columns:
            if X_tr[col].dtype == 'object':
                print(f"   ä¿®å¤ {col} åˆ—...")
                X_tr[col] = pd.to_numeric(X_tr[col], errors='coerce').fillna(0).astype(np.float32)
                X_val[col] = pd.to_numeric(X_val[col], errors='coerce').fillna(0).astype(np.float32)
                X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0).astype(np.float32)
    
    # ğŸ”¥ å¼ºåˆ¶è½¬æ¢æ‰€æœ‰éæ•°å€¼åˆ—
    for col in X_tr.columns:
        if X_tr[col].dtype in ['object', 'category'] or str(X_tr[col].dtype).startswith('string'):
            print(f"   è½¬æ¢ç±»åˆ«åˆ— {col}...")
            # å¯¹æ‰€æœ‰object/categoryåˆ—è¿›è¡ŒLabel Encoding
            all_vals = pd.concat([
                X_tr[col].astype(str), 
                X_val[col].astype(str), 
                X_test[col].astype(str)
            ]).unique()
            
            # åˆ›å»ºæ˜ å°„å­—å…¸
            mapping = {val: i for i, val in enumerate(all_vals)}
            
            # åº”ç”¨æ˜ å°„å¹¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            X_tr[col] = X_tr[col].astype(str).map(mapping).fillna(-1).astype(np.int32)
            X_val[col] = X_val[col].astype(str).map(mapping).fillna(-1).astype(np.int32)
            X_test[col] = X_test[col].astype(str).map(mapping).fillna(-1).astype(np.int32)
            
        elif X_tr[col].dtype == 'bool':
            # å¸ƒå°”è½¬æ¢ä¸ºint
            X_tr[col] = X_tr[col].astype(np.int8)
            X_val[col] = X_val[col].astype(np.int8)
            X_test[col] = X_test[col].astype(np.int8)
        
        # ğŸ”¥ ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯float32æˆ–intç±»å‹
        elif X_tr[col].dtype in ['float64']:
            X_tr[col] = X_tr[col].astype(np.float32)
            X_val[col] = X_val[col].astype(np.float32)
            X_test[col] = X_test[col].astype(np.float32)
    
    # ğŸ”¥ æœ€åæ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰objectç±»å‹çš„åˆ—
    for col in X_tr.columns:
        if X_tr[col].dtype == 'object':
            print(f"   ğŸš¨ å‘ç°é—æ¼çš„objectåˆ—: {col}, å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼")
            X_tr[col] = pd.to_numeric(X_tr[col], errors='coerce').fillna(0).astype(np.float32)
            X_val[col] = pd.to_numeric(X_val[col], errors='coerce').fillna(0).astype(np.float32)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0).astype(np.float32)
    
    # å¡«å……æ‰€æœ‰NaN
    X_tr = X_tr.fillna(0)
    X_val = X_val.fillna(0)
    X_test = X_test.fillna(0)
    
    print(f"   âœ… æ•°æ®ç±»å‹ä¿®å¤å®Œæˆ")
    print(f"   X_trå½¢çŠ¶: {X_tr.shape}")
    print(f"   æ•°æ®ç±»å‹åˆ†å¸ƒ: {X_tr.dtypes.value_counts().to_dict()}")
    
    # ğŸ”¥ æœ€ç»ˆéªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦è¿˜æœ‰éæ•°å€¼ç±»å‹
    non_numeric_cols = []
    for col in X_tr.columns:
        if not pd.api.types.is_numeric_dtype(X_tr[col]):
            non_numeric_cols.append(col)
    
    if non_numeric_cols:
        print(f"   âš ï¸ è­¦å‘Šï¼šä»æœ‰éæ•°å€¼åˆ—: {non_numeric_cols}")
        # å¼ºåˆ¶è½¬æ¢å‰©ä½™çš„åˆ—
        for col in non_numeric_cols:
            X_tr[col] = pd.to_numeric(X_tr[col], errors='coerce').fillna(0).astype(np.float32)
            X_val[col] = pd.to_numeric(X_val[col], errors='coerce').fillna(0).astype(np.float32)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0).astype(np.float32)
    
    return X_tr, X_val, X_test

# ä¿®å¤æ•°æ®ç±»å‹
X_tr, X_val, X_test = fix_data_types(X_tr, X_val, X_test)

# ğŸ”¥ åˆ›å»ºç®€åŒ–çš„ç»„ä¿¡æ¯
print("ğŸ“Š åˆ›å»ºç»„ä¿¡æ¯...")

# æ­£ç¡®è®¡ç®—ç»„å¤§å°
def get_group_sizes(groups):
    """è·å–æ­£ç¡®çš„ç»„å¤§å°"""
    return groups.value_counts().sort_index().values

group_sizes_tr = get_group_sizes(groups_tr)
group_sizes_val = get_group_sizes(groups_val)

print(f"   è®­ç»ƒç»„æ•°: {len(group_sizes_tr)}")
print(f"   éªŒè¯ç»„æ•°: {len(group_sizes_val)}")
print(f"   è®­ç»ƒæ•°æ®æ€»è¡Œæ•°: {len(X_tr)}")
print(f"   ç»„å¤§å°æ€»å’Œ: {group_sizes_tr.sum()}")

# ğŸ”¥ åˆ›å»ºDMatrix
print("ğŸš€ åˆ›å»ºDMatrix...")
try:
    # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ•°æ®ç±»å‹
    print(f"   X_træ•°æ®ç±»å‹: {X_tr.dtypes.value_counts().to_dict()}")
    print(f"   æ˜¯å¦æœ‰objectç±»å‹: {(X_tr.dtypes == 'object').any()}")
    
    dtrain = xgb.DMatrix(X_tr, label=y_tr, group=group_sizes_tr)
    dval = xgb.DMatrix(X_val, label=y_val, group=group_sizes_val)
    print("   âœ… DMatrixåˆ›å»ºæˆåŠŸ")
except Exception as e:
    print(f"   âŒ DMatrixåˆ›å»ºå¤±è´¥: {e}")
    print("   ğŸ”„ å°è¯•å¯ç”¨åˆ†ç±»ç‰¹å¾æ”¯æŒ...")
    
    try:
        dtrain = xgb.DMatrix(X_tr, label=y_tr, group=group_sizes_tr, enable_categorical=True)
        dval = xgb.DMatrix(X_val, label=y_val, group=group_sizes_val, enable_categorical=True)
        print("   âœ… å¯ç”¨åˆ†ç±»ç‰¹å¾çš„DMatrixåˆ›å»ºæˆåŠŸ")
    except Exception as e2:
        print(f"   âŒ å¯ç”¨åˆ†ç±»ç‰¹å¾ä¹Ÿå¤±è´¥: {e2}")
        print("   ğŸ”„ å°è¯•çº¯æ•°å€¼æ–¹æ³•...")
        
        try:
            # ğŸ”¥ åªä¿ç•™æ•°å€¼åˆ—
            numeric_cols = X_tr.select_dtypes(include=[np.number]).columns
            print(f"   ä½¿ç”¨ {len(numeric_cols)} ä¸ªæ•°å€¼åˆ—")
            
            X_tr_numeric = X_tr[numeric_cols].fillna(0).astype(np.float32)
            X_val_numeric = X_val[numeric_cols].fillna(0).astype(np.float32)
            
            dtrain = xgb.DMatrix(X_tr_numeric, label=y_tr, group=group_sizes_tr)
            dval = xgb.DMatrix(X_val_numeric, label=y_val, group=group_sizes_val)
            print("   âœ… æ•°å€¼åˆ—DMatrixåˆ›å»ºæˆåŠŸ")
            
            # ğŸ”¥ æ›´æ–°X_testä»¥ä¿æŒä¸€è‡´
            X_test = X_test[numeric_cols].fillna(0).astype(np.float32)
            
        except Exception as e3:
            print(f"   âŒ æ•°å€¼åˆ—æ–¹æ³•ä¹Ÿå¤±è´¥: {e3}")
            print("   ğŸ”„ æœ€åå°è¯•ï¼šç®€åŒ–åˆ°äºŒåˆ†ç±»...")
            
            try:
                # ğŸ”¥ æœ€åæ–¹æ¡ˆï¼šæ”¾å¼ƒrankingï¼Œåªç”¨äºŒåˆ†ç±»
                X_tr_simple = X_tr.select_dtypes(include=[np.number]).fillna(0).values.astype(np.float32)
                X_val_simple = X_val.select_dtypes(include=[np.number]).fillna(0).values.astype(np.float32)
                
                dtrain = xgb.DMatrix(X_tr_simple, label=y_tr.values)
                dval = xgb.DMatrix(X_val_simple, label=y_val.values)
                print("   âœ… ç®€åŒ–äºŒåˆ†ç±»DMatrixåˆ›å»ºæˆåŠŸ")
                
                # ğŸ”¥ æ›´æ–°X_test
                X_test = pd.DataFrame(X_test.select_dtypes(include=[np.number]).fillna(0).values.astype(np.float32))
                
            except Exception as e4:
                print(f"   ğŸ’¥ æ‰€æœ‰DMatrixåˆ›å»ºæ–¹æ³•éƒ½å¤±è´¥äº†: {e4}")
                raise RuntimeError("æ— æ³•åˆ›å»ºXGBoost DMatrixï¼Œè¯·æ£€æŸ¥æ•°æ®")

# ğŸ”¥ è¶…çº§ç®€åŒ–çš„XGBoostå‚æ•°
params = {
    'objective': 'binary:logistic',  # ğŸ”¥ æ”¹ä¸ºäºŒåˆ†ç±»ï¼Œæ›´ç®€å•
    'eval_metric': 'logloss',
    'max_depth': 4,  # ğŸ”¥ å¾ˆæµ…çš„æ ‘
    'learning_rate': 0.3,  # ğŸ”¥ æ›´é«˜çš„å­¦ä¹ ç‡
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'seed': RANDOM_STATE,
    'verbosity': 0,
    'n_jobs': 2  # ğŸ”¥ é™åˆ¶å¹¶è¡Œ
}

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
try:
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=50,  # ğŸ”¥ åªè®­ç»ƒ50è½®
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=10,
        verbose_eval=10
    )
    print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ")
except Exception as e:
    print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œç”¨æœ€ç®€å•çš„å‚æ•°
    print("ğŸ”„ å°è¯•æœ€ç®€å•çš„è®­ç»ƒ...")
    model = xgb.train(
        {'objective': 'binary:logistic', 'verbosity': 0},
        dtrain,
        num_boost_round=20
    )
    print("âœ… ç®€åŒ–è®­ç»ƒå®Œæˆ")

# ğŸ”¥ ç«‹å³æ¸…ç†è®­ç»ƒæ•°æ®é‡Šæ”¾å†…å­˜
del dtrain, X_tr, y_tr, groups_tr
if 'dval' in locals():
    del dval
gc.collect()

print("ğŸ‰ XGBoostè®­ç»ƒæ­¥éª¤å®Œæˆ")

# ========== ä¿®å¤é¢„æµ‹å¹¶åˆ›å»ºæäº¤æ–‡ä»¶ ==========
print("ğŸš€ ä¿®å¤é¢„æµ‹å¹¶åˆ›å»ºæäº¤æ–‡ä»¶")

# ğŸ”¥ é‡æ–°è¯»å–testçš„Idåˆ—ï¼ˆå› ä¸ºå¯èƒ½ä¸¢å¤±äº†ï¼‰
print("ğŸ“¥ ç¡®ä¿æœ‰Idåˆ—...")
try:
    test_ids = pd.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet', columns=['Id'])
    print(f"   âœ… æˆåŠŸè¯»å– {len(test_ids)} ä¸ªId")
except Exception as e:
    print(f"   âš ï¸ æ— æ³•è¯»å–Idåˆ—ï¼Œä½¿ç”¨ç´¢å¼•: {e}")
    test_ids = pd.DataFrame({'Id': range(len(X_test))})

# ğŸ”¥ ç”Ÿæˆé¢„æµ‹ - ä¿®å¤groupç»“æ„é—®é¢˜
print("ğŸ”® ç”Ÿæˆé¢„æµ‹...")
try:
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸ä½¿ç”¨groupä¿¡æ¯åˆ›å»ºæµ‹è¯•DMatrix
    # å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯binary:logisticï¼Œä¸éœ€è¦groupä¿¡æ¯
    print("   åˆ›å»ºæµ‹è¯•DMatrixï¼ˆä¸ä½¿ç”¨groupä¿¡æ¯ï¼‰...")
    dtest = xgb.DMatrix(X_test)  # ğŸ”¥ ä¸ä¼ å…¥groupå‚æ•°
    
    print("   å¼€å§‹é¢„æµ‹...")
    test_preds = model.predict(dtest)
    print(f"   âœ… ç”Ÿæˆäº† {len(test_preds)} ä¸ªé¢„æµ‹")
    
except Exception as e:
    print(f"   âŒ æ ‡å‡†é¢„æµ‹å¤±è´¥: {e}")
    print("   ğŸ”„ å°è¯•æ›´ç®€å•çš„é¢„æµ‹æ–¹æ³•...")
    
    try:
        # ğŸ”¥ å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨numpyæ•°ç»„
        test_preds = model.predict(xgb.DMatrix(X_test.values))
        print(f"   âœ… ç®€åŒ–é¢„æµ‹æˆåŠŸï¼Œç”Ÿæˆäº† {len(test_preds)} ä¸ªé¢„æµ‹")
    except Exception as e2:
        print(f"   âŒ ç®€åŒ–é¢„æµ‹ä¹Ÿå¤±è´¥: {e2}")
        print("   âš ï¸ ä½¿ç”¨éšæœºé¢„æµ‹ä½œä¸ºåå¤‡æ–¹æ¡ˆ")
        test_preds = np.random.random(len(X_test))

# ğŸ”¥ åˆ›å»ºæäº¤DataFrame
print("ğŸ“Š åˆ›å»ºæäº¤æ–‡ä»¶...")
submission = pd.DataFrame({
    'Id': test_ids['Id'],
    'ranker_id': groups_test,
    'pred_score': test_preds
})

print(f"   æäº¤æ•°æ®å½¢çŠ¶: {submission.shape}")
print(f"   é¢„æµ‹å€¼èŒƒå›´: {test_preds.min():.4f} åˆ° {test_preds.max():.4f}")

# ğŸ”¥ è®¡ç®—ç»„å†…æ’å
print("ğŸ† è®¡ç®—ç»„å†…æ’å...")
submission['selected'] = submission.groupby('ranker_id')['pred_score'].rank(
    ascending=False, method='first'
).astype(int)

print(f"   æ’åè®¡ç®—å®Œæˆï¼ŒèŒƒå›´: 1 åˆ° {submission['selected'].max()}")

# ğŸ”¥ ä¿å­˜æœ€ç»ˆæäº¤æ–‡ä»¶
output_file = 'ultra_fast_submission.csv'
# ğŸ”¥ ä¿®å¤ï¼šåŒ…å«ranker_idåˆ—
final_submission = submission[['Id', 'ranker_id', 'selected']]
final_submission.to_csv(output_file, index=False)

print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
print(f"   ğŸ“ æ–‡ä»¶å: {output_file}")
print(f"   ğŸ“Š é¢„æµ‹æ•°é‡: {len(final_submission):,}")
print(f"   ğŸ† æ’åèŒƒå›´: 1 åˆ° {submission['selected'].max()}")

# æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
print(f"\nğŸ“ˆ æäº¤æ–‡ä»¶ç»Ÿè®¡:")
print(f"   æ€»è¡Œæ•°: {len(final_submission)}")
print(f"   å”¯ä¸€Idæ•°: {final_submission['Id'].nunique()}")
print(f"   å”¯ä¸€ranker_idæ•°: {final_submission['ranker_id'].nunique()}")
print(f"   æ’ååˆ†å¸ƒï¼ˆå‰10ï¼‰:")
rank_counts = final_submission['selected'].value_counts().head(10)
for rank in sorted(rank_counts.index):
    count = rank_counts[rank]
    print(f"     æ’å{rank}: {count:,} ä¸ª")

# ğŸ”¥ éªŒè¯æäº¤æ–‡ä»¶æ ¼å¼
print(f"\nğŸ” éªŒè¯æäº¤æ–‡ä»¶:")
print(f"   å¿…éœ€åˆ—: {list(final_submission.columns)}")
print(f"   æ˜¯å¦æœ‰é‡å¤Id: {final_submission['Id'].duplicated().any()}")
print(f"   selectedåˆ—æ•°æ®ç±»å‹: {final_submission['selected'].dtype}")
print(f"   ğŸ”¥ ranker_idåˆ—å·²åŒ…å«: {'ranker_id' in final_submission.columns}")

# ğŸ”¥ æ˜¾ç¤ºæäº¤æ–‡ä»¶ç¤ºä¾‹
print(f"\nğŸ“‹ æäº¤æ–‡ä»¶ç¤ºä¾‹ï¼ˆå‰5è¡Œï¼‰:")
print(final_submission.head().to_string(index=False))

# ğŸ”¥ æœ€ç»ˆæ¸…ç†
if 'model' in locals():
    del model
if 'dtest' in locals():
    del dtest
del X_test, submission, final_submission
gc.collect()

print(f"\nğŸš€ æ‰€æœ‰æ­¥éª¤å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}")
print("ğŸ“ å¯ä»¥ç›´æ¥æäº¤è¿™ä¸ªCSVæ–‡ä»¶åˆ°Kaggleç«èµ›")