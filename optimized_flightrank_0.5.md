#cursorÁîüÊàê

# üöÄ KaggleÂàÜÊï∞ÊèêÂçáÁ≠ñÁï• - ‰ªé0.47Âà∞0.5+ÂàÜ

## üìä Áé∞Áä∂ÂàÜÊûê

‰ªébaseline‰ª£Á†ÅÂàÜÊûêÔºåÂΩìÂâçÊ®°ÂûãÂ∑≤ÁªèËææÂà∞‰∫Ü**È™åËØÅÈõÜHitRate@3: 0.5042**ÁöÑ‰∏çÈîôË°®Áé∞„ÄÇ‰ΩÜË¶ÅÂú®Kaggle‰∏äËé∑ÂæóÊõ¥È´òÂàÜÊï∞ÔºåÈúÄË¶ÅÁ≥ªÁªüÊÄß‰ºòÂåñ„ÄÇ

### üîç ÂΩìÂâçÊ®°Âûã‰ºòÂäø
- ‚úÖ ‰ΩøÁî®‰∫Ü112‰∏™ÁâπÂæÅÔºåÁâπÂæÅÂ∑•Á®ãËæÉ‰∏∫ÂÆåÂñÑ
- ‚úÖ XGBoostÊéíÂ∫èÊ®°ÂûãÈÖçÁΩÆÂêàÁêÜ
- ‚úÖ È™åËØÅÈõÜË°®Áé∞ËâØÂ•Ω (0.5042)
- ‚úÖ ÂÜÖÂ≠ò‰ΩøÁî®ÊéßÂà∂Âú®ÂÆâÂÖ®ËåÉÂõ¥ÂÜÖ

### ‚ö†Ô∏è ÊΩúÂú®ÊèêÂçáÁ©∫Èó¥
1. **Êï∞ÊçÆÈááÊ†∑**: ÁõÆÂâçÂè™Áî®50%Êï∞ÊçÆÔºåÂèØ‰ª•ÈÄÇÂΩìÂ¢ûÂä†
2. **ÁâπÂæÅ‰ºòÂåñ**: ÂèØ‰ª•Ê∑ªÂä†Êõ¥Â§ö‰∫§‰∫íÁâπÂæÅ
3. **Ê®°ÂûãÈõÜÊàê**: ÂçïÊ®°ÂûãÂ≠òÂú®Â±ÄÈôêÊÄß
4. **ÂèÇÊï∞Ë∞É‰ºò**: ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñXGBoostÂèÇÊï∞
5. **ÂêéÂ§ÑÁêÜ**: ÂèØ‰ª•‰ºòÂåñÊéíÂ∫èÁ≠ñÁï•

---

## üéØ ‰∫îÂ§ßÊèêÂçáÁ≠ñÁï•

### 1Ô∏è‚É£ Êï∞ÊçÆÈááÊ†∑‰ºòÂåñ

```python
# üî• Á≠ñÁï•1: ÈÄÇÂ∫¶Â¢ûÂä†Êï∞ÊçÆÈááÊ†∑
# ÂéüÂßã: 50% ‚Üí Âª∫ËÆÆ: 65-70%

# Âú®ÈÖçÁΩÆÈÉ®ÂàÜ‰øÆÊîπ:
TRAIN_SAMPLE_FRAC = 0.65  # ‰ªé0.5ÊèêÂçáÂà∞0.65

# È¢ÑÊúüÊïàÊûú: +0.02-0.03ÂàÜ
# ÂÜÖÂ≠òÊ∂àËÄó: Á∫¶10-11GB (ÂÆâÂÖ®ËåÉÂõ¥)
```

### 2Ô∏è‚É£ ÁâπÂæÅÂ∑•Á®ãÂ¢ûÂº∫

```python
# üî• Á≠ñÁï•2: Ê∑ªÂä†È´ò‰ª∑ÂÄº‰∫§‰∫íÁâπÂæÅ

def create_enhanced_features(df):
    """Â¢ûÂº∫ÁâπÂæÅÂ∑•Á®ã"""
    df = df.copy()
    
    # Áé∞ÊúâÁâπÂæÅÂ∑•Á®ã‰ª£Á†Å...
    
    # üî• Êñ∞Â¢û‰∫§‰∫íÁâπÂæÅ
    feat = {}
    
    # ‰ª∑Ê†º-Êó∂Èïø‰∫§‰∫í
    feat["price_per_hour"] = df["totalPrice"] / (df["total_duration"] + 1)
    feat["price_duration_ratio"] = df["totalPrice"] * df["total_duration"] / 1000
    
    # Ëà™Âè∏-Ë∑ØÁ∫ø‰∫§‰∫í
    feat["carrier_route"] = (
        df["legs0_segments0_marketingCarrier_code"].astype(str) + "_" + 
        df["searchRoute"].astype(str)
    )
    
    # Êó∂Èó¥-‰ª∑Ê†º‰∫§‰∫í
    feat["weekend_premium"] = (
        (df["legs0_departureAt_weekday"] >= 5) * 
        (df["price_pct_rank"] > 0.7)
    ).astype(int)
    
    # Ëà±‰Ωç-‰ª∑Ê†º‰∫§‰∫í
    feat["cabin_price_match"] = (
        (df["legs0_segments0_cabinClass"] == 1) & 
        (df["price_pct_rank"] < 0.3)
    ).astype(int)
    
    # Áõ¥È£û-‰ª∑Ê†º‰ºòÂäø
    feat["direct_price_advantage"] = (
        feat["is_direct_leg0"] * 
        (1 - df["price_pct_rank"])
    )
    
    # Â∏∏ÂÆ¢-Ëà™Âè∏ÂåπÈÖçÂ∫¶
    for airline in ["SU", "S7", "U6"]:
        if f"ff_{airline}" in feat:
            feat[f"ff_{airline}_match"] = (
                feat[f"ff_{airline}"] * 
                (df["legs0_segments0_marketingCarrier_code"] == airline)
            ).astype(int)
    
    # È¢ÑÊúüÊïàÊûú: +0.015-0.025ÂàÜ
    return pd.concat([df, pd.DataFrame(feat, index=df.index)], axis=1)
```

### 3Ô∏è‚É£ Ê®°ÂûãÂèÇÊï∞‰ºòÂåñ

```python
# üî• Á≠ñÁï•3: Á≤æÁªÜË∞É‰ºòXGBoostÂèÇÊï∞

# ÂéüÂßãÂèÇÊï∞
xgb_params_original = {
    'max_depth': 8,
    'learning_rate': 0.05,
    'lambda': 10.0,
    'num_boost_round': 1500
}

# üî• ‰ºòÂåñÂèÇÊï∞
xgb_params_optimized = {
    'objective': 'rank:pairwise',
    'eval_metric': 'ndcg@3',
    'max_depth': 9,                    # Á®çÂæÆÂ¢ûÂä†Ê∑±Â∫¶
    'min_child_weight': 8,             # Èôç‰ΩéÊúÄÂ∞èÂ≠êËäÇÁÇπÊùÉÈáç
    'subsample': 0.85,                 # Â¢ûÂä†Â≠êÈááÊ†∑
    'colsample_bytree': 0.85,          # Â¢ûÂä†ÁâπÂæÅÈááÊ†∑
    'lambda': 8.0,                     # Èôç‰ΩéL2Ê≠£ÂàôÂåñ
    'alpha': 2.0,                      # Ê∑ªÂä†L1Ê≠£ÂàôÂåñ
    'learning_rate': 0.04,             # Èôç‰ΩéÂ≠¶‰π†Áéá
    'seed': RANDOM_STATE,
    'n_jobs': -1,
    'tree_method': 'hist',             # ‰ΩøÁî®Áõ¥ÊñπÂõæÁÆóÊ≥ï
    'max_bin': 512                     # Â¢ûÂä†ÂàÜÁÆ±Êï∞
}

# ËÆ≠ÁªÉËΩÆÊï∞Ë∞ÉÊï¥
num_boost_round = 2000                 # Â¢ûÂä†ËÆ≠ÁªÉËΩÆÊï∞
early_stopping_rounds = 150            # Â¢ûÂä†Êó©ÂÅúËΩÆÊï∞

# È¢ÑÊúüÊïàÊûú: +0.01-0.02ÂàÜ
```

### 4Ô∏è‚É£ Ê®°ÂûãÈõÜÊàêÁ≠ñÁï•

```python
# üî• Á≠ñÁï•4: ËΩªÈáèÁ∫ßÊ®°ÂûãÈõÜÊàê

def create_ensemble_model():
    """ÂàõÂª∫ÈõÜÊàêÊ®°Âûã"""
    
    # Ê®°Âûã1: ÂéüÂßãXGBoost
    xgb_model1 = xgb.train(xgb_params_optimized, dtrain, 
                          num_boost_round=2000, 
                          early_stopping_rounds=150)
    
    # Ê®°Âûã2: ‰∏çÂêåÈöèÊú∫ÁßçÂ≠êÁöÑXGBoost
    xgb_params2 = xgb_params_optimized.copy()
    xgb_params2['seed'] = 2024
    xgb_params2['subsample'] = 0.8
    xgb_params2['colsample_bytree'] = 0.8
    
    xgb_model2 = xgb.train(xgb_params2, dtrain,
                          num_boost_round=1800,
                          early_stopping_rounds=150)
    
    # Ê®°Âûã3: Êõ¥‰øùÂÆàÁöÑÂèÇÊï∞
    xgb_params3 = xgb_params_optimized.copy()
    xgb_params3['max_depth'] = 7
    xgb_params3['learning_rate'] = 0.06
    xgb_params3['lambda'] = 12.0
    
    xgb_model3 = xgb.train(xgb_params3, dtrain,
                          num_boost_round=1500,
                          early_stopping_rounds=100)
    
    return [xgb_model1, xgb_model2, xgb_model3]

def ensemble_predict(models, dtest):
    """ÈõÜÊàêÈ¢ÑÊµã"""
    predictions = []
    weights = [0.5, 0.3, 0.2]  # ÊùÉÈáçÂàÜÈÖç
    
    for model in models:
        pred = model.predict(dtest)
        predictions.append(pred)
    
    # Âä†ÊùÉÂπ≥Âùá
    ensemble_pred = np.average(predictions, axis=0, weights=weights)
    return ensemble_pred

# È¢ÑÊúüÊïàÊûú: +0.02-0.04ÂàÜ
```

### 5Ô∏è‚É£ ÂêéÂ§ÑÁêÜ‰ºòÂåñ

```python
# üî• Á≠ñÁï•5: Êô∫ËÉΩÂêéÂ§ÑÁêÜ

def smart_post_processing(submission_df):
    """Êô∫ËÉΩÂêéÂ§ÑÁêÜ‰ºòÂåñ"""
    
    # 1. ‰ª∑Ê†ºÂêàÁêÜÊÄßÊ£ÄÊü•
    # Á°Æ‰øùÊúÄ‰æøÂÆúÁöÑÈÄâÈ°πËé∑ÂæóÊõ¥È´òÊéíÂêç
    def price_adjustment(group):
        if len(group) > 1:
            # Â¶ÇÊûúÊúÄ‰æøÂÆúÁöÑ‰∏çÂú®Ââç3ÔºåÈÄÇÂΩìÊèêÂçá
            cheapest_idx = group['totalPrice'].idxmin()
            if group.loc[cheapest_idx, 'selected'] > 3:
                # ËΩªÂæÆÊèêÂçáÊúÄ‰æøÂÆúÈÄâÈ°πÁöÑÂàÜÊï∞
                group.loc[cheapest_idx, 'pred_score'] *= 1.1
        return group
    
    # 2. Áõ¥È£ûÂÅèÂ•ΩË∞ÉÊï¥
    def direct_flight_boost(group):
        # Â¶ÇÊûúÊúâÁõ¥È£ûÈÄâÈ°πÔºåËΩªÂæÆÊèêÂçáÂÖ∂ÂàÜÊï∞
        direct_mask = group['is_direct_leg0'] == 1
        if direct_mask.any():
            group.loc[direct_mask, 'pred_score'] *= 1.05
        return group
    
    # 3. Â∏∏ÂÆ¢ÂåπÈÖçË∞ÉÊï¥
    def frequent_flyer_boost(group):
        # Â¶ÇÊûúÂ∏∏ÂÆ¢ËÆ°ÂàíÂåπÈÖçËà™Âè∏ÔºåÊèêÂçáÂàÜÊï∞
        for airline in ['SU', 'S7', 'U6']:
            ff_match = (group[f'ff_{airline}'] == 1) & \
                      (group['legs0_segments0_marketingCarrier_code'] == airline)
            if ff_match.any():
                group.loc[ff_match, 'pred_score'] *= 1.03
        return group
    
    # Â∫îÁî®ÂêéÂ§ÑÁêÜ
    processed = submission_df.groupby('ranker_id').apply(price_adjustment)
    processed = processed.groupby('ranker_id').apply(direct_flight_boost)
    processed = processed.groupby('ranker_id').apply(frequent_flyer_boost)
    
    # ÈáçÊñ∞ÊéíÂ∫è
    processed['selected'] = processed.groupby('ranker_id')['pred_score'].rank(
        ascending=False, method='first'
    ).astype(int)
    
    return processed

# È¢ÑÊúüÊïàÊûú: +0.01-0.015ÂàÜ
```

---

## üî• ÂÆåÊï¥ÂÆûÊñΩ‰ª£Á†Å

```python
# ========== KaggleÂàÜÊï∞ÊèêÂçáÂÆåÊï¥‰ª£Á†Å ==========

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import log_loss
import gc
import warnings
warnings.filterwarnings('ignore')

# üî• ‰ºòÂåñÈÖçÁΩÆ
TRAIN_SAMPLE_FRAC = 0.65  # ÊèêÂçáÂà∞65%
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("üöÄ KaggleÂàÜÊï∞ÊèêÂçáÁ≠ñÁï• v4.0")
print(f"üìä ÁõÆÊ†á: ‰ªé0.41ÊèêÂçáÂà∞0.52+")

# 1. Êï∞ÊçÆÂä†ËΩΩ (‰ΩøÁî®ÂéüÊúâ‰ª£Á†Å)
# ... ÂéüÊúâÁöÑÊï∞ÊçÆÂä†ËΩΩ‰ª£Á†Å ...

# 2. Â¢ûÂº∫ÁâπÂæÅÂ∑•Á®ã
def create_enhanced_features(df):
    """Â¢ûÂº∫ÁâàÁâπÂæÅÂ∑•Á®ã"""
    # ‰ΩøÁî®ÂéüÊúâÁöÑcreate_featuresÂáΩÊï∞
    df = create_features(df)
    
    # Ê∑ªÂä†Êñ∞ÁöÑ‰∫§‰∫íÁâπÂæÅ
    feat = {}
    
    # ‰ª∑Ê†º-Êó∂Èïø‰∫§‰∫í
    feat["price_per_hour"] = df["totalPrice"] / (df["total_duration"] + 1)
    feat["price_duration_interaction"] = df["totalPrice"] * df["total_duration"] / 1000
    
    # Ëà™Âè∏-Ë∑ØÁ∫ø‰∫§‰∫í (‰ΩøÁî®label encoding)
    carrier_route = (
        df["legs0_segments0_marketingCarrier_code"].astype(str) + "_" + 
        df["searchRoute"].astype(str)
    )
    unique_combinations = carrier_route.unique()
    carrier_route_map = {combo: idx for idx, combo in enumerate(unique_combinations)}
    feat["carrier_route_encoded"] = carrier_route.map(carrier_route_map)
    
    # Êó∂Èó¥-‰ª∑Ê†º‰∫§‰∫í
    if "legs0_departureAt_weekday" in df.columns:
        feat["weekend_premium"] = (
            (df["legs0_departureAt_weekday"] >= 5) * 
            (df["price_pct_rank"] > 0.7)
        ).astype(int)
    
    # Ëà±‰Ωç-‰ª∑Ê†ºÂåπÈÖç
    feat["economy_cheap"] = (
        (df["legs0_segments0_cabinClass"] == 1) & 
        (df["price_pct_rank"] < 0.3)
    ).astype(int)
    
    # Áõ¥È£û‰ª∑Ê†º‰ºòÂäø
    if "is_direct_leg0" in df.columns:
        feat["direct_price_advantage"] = (
            df["is_direct_leg0"] * (1 - df["price_pct_rank"])
        )
    
    # Â∏∏ÂÆ¢Ëà™Âè∏ÂåπÈÖç
    for airline in ["SU", "S7", "U6"]:
        if f"ff_{airline}" in df.columns:
            feat[f"ff_{airline}_carrier_match"] = (
                df[f"ff_{airline}"] * 
                (df["legs0_segments0_marketingCarrier_code"] == airline)
            ).astype(int)
    
    # Â∫ß‰ΩçÁ®ÄÁº∫ÊÄß
    feat["seat_scarcity"] = 1 / (df["legs0_segments0_seatsAvailable"].fillna(100) + 1)
    
    # ‰ª∑Ê†ºÂàÜÂ∏ÉÁâπÂæÅ
    grp = df.groupby("ranker_id")
    feat["price_std"] = grp["totalPrice"].transform("std").fillna(0)
    feat["price_range"] = grp["totalPrice"].transform(lambda x: x.max() - x.min())
    feat["price_median_diff"] = df["totalPrice"] - grp["totalPrice"].transform("median")
    
    # Êó∂ÈïøÂàÜÂ∏ÉÁâπÂæÅ
    feat["duration_std"] = grp["total_duration"].transform("std").fillna(0)
    feat["duration_median_diff"] = df["total_duration"] - grp["total_duration"].transform("median")
    
    return pd.concat([df, pd.DataFrame(feat, index=df.index)], axis=1)

# 3. Â∫îÁî®Â¢ûÂº∫ÁâπÂæÅÂ∑•Á®ã
print("üîß Â∫îÁî®Â¢ûÂº∫ÁâπÂæÅÂ∑•Á®ã...")
train = create_enhanced_features(train)
test = create_enhanced_features(test)

# 4. ‰ºòÂåñÂêéÁöÑÊ®°ÂûãÂèÇÊï∞
xgb_params_optimized = {
    'objective': 'rank:pairwise',
    'eval_metric': 'ndcg@3',
    'max_depth': 9,
    'min_child_weight': 8,
    'subsample': 0.85,
    'colsample_bytree': 0.85,
    'lambda': 8.0,
    'alpha': 2.0,
    'learning_rate': 0.04,
    'seed': RANDOM_STATE,
    'n_jobs': -1,
    'tree_method': 'hist',
    'max_bin': 512
}

# 5. ËÆ≠ÁªÉÈõÜÊàêÊ®°Âûã
def train_ensemble_models():
    """ËÆ≠ÁªÉÈõÜÊàêÊ®°Âûã"""
    models = []
    
    # Ê®°Âûã1: ‰∏ªÊ®°Âûã
    print("üèãÔ∏è ËÆ≠ÁªÉ‰∏ªÊ®°Âûã...")
    model1 = xgb.train(
        xgb_params_optimized,
        dtrain,
        num_boost_round=2000,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=150,
        verbose_eval=100
    )
    models.append(model1)
    
    # Ê®°Âûã2: ‰∏çÂêåÈöèÊú∫ÁßçÂ≠ê
    print("üèãÔ∏è ËÆ≠ÁªÉËæÖÂä©Ê®°Âûã1...")
    params2 = xgb_params_optimized.copy()
    params2['seed'] = 2024
    params2['subsample'] = 0.8
    params2['colsample_bytree'] = 0.8
    
    model2 = xgb.train(
        params2,
        dtrain,
        num_boost_round=1800,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=150,
        verbose_eval=100
    )
    models.append(model2)
    
    # Ê®°Âûã3: ‰øùÂÆàÂèÇÊï∞
    print("üèãÔ∏è ËÆ≠ÁªÉËæÖÂä©Ê®°Âûã2...")
    params3 = xgb_params_optimized.copy()
    params3['max_depth'] = 7
    params3['learning_rate'] = 0.06
    params3['lambda'] = 12.0
    
    model3 = xgb.train(
        params3,
        dtrain,
        num_boost_round=1500,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=100,
        verbose_eval=100
    )
    models.append(model3)
    
    return models

# 6. ÈõÜÊàêÈ¢ÑÊµã
def ensemble_predict(models, dtest):
    """ÈõÜÊàêÈ¢ÑÊµã"""
    predictions = []
    weights = [0.5, 0.3, 0.2]
    
    for i, model in enumerate(models):
        pred = model.predict(dtest)
        predictions.append(pred)
        print(f"‚úÖ Ê®°Âûã{i+1}È¢ÑÊµãÂÆåÊàê")
    
    ensemble_pred = np.average(predictions, axis=0, weights=weights)
    return ensemble_pred

# 7. Êô∫ËÉΩÂêéÂ§ÑÁêÜ
def smart_post_processing(df):
    """Êô∫ËÉΩÂêéÂ§ÑÁêÜ"""
    def group_adjustment(group):
        # ‰ª∑Ê†ºÂêàÁêÜÊÄßË∞ÉÊï¥
        if len(group) > 1:
            cheapest_idx = group['totalPrice'].idxmin()
            if group.loc[cheapest_idx, 'selected'] > 3:
                group.loc[cheapest_idx, 'pred_score'] *= 1.08
        
        # Áõ¥È£ûÂÅèÂ•ΩË∞ÉÊï¥
        if 'is_direct_leg0' in group.columns:
            direct_mask = group['is_direct_leg0'] == 1
            if direct_mask.any():
                group.loc[direct_mask, 'pred_score'] *= 1.04
        
        return group
    
    processed = df.groupby('ranker_id').apply(group_adjustment)
    
    # ÈáçÊñ∞ÊéíÂ∫è
    processed['selected'] = processed.groupby('ranker_id')['pred_score'].rank(
        ascending=False, method='first'
    ).astype(int)
    
    return processed

# 8. ÊâßË°åÂÆåÊï¥ÊµÅÁ®ã
print("üöÄ ÂºÄÂßãÂÆåÊï¥‰ºòÂåñÊµÅÁ®ã...")

# ËÆ≠ÁªÉÈõÜÊàêÊ®°Âûã
models = train_ensemble_models()

# ÁîüÊàêÈõÜÊàêÈ¢ÑÊµã
print("üîÆ ÁîüÊàêÈõÜÊàêÈ¢ÑÊµã...")
dtest = xgb.DMatrix(X_test_xgb, group=group_sizes_test)
ensemble_preds = ensemble_predict(models, dtest)

# ÂàõÂª∫Êèê‰∫§Êñá‰ª∂
submission_enhanced = test[['Id', 'ranker_id', 'totalPrice']].copy()
submission_enhanced['pred_score'] = ensemble_preds
submission_enhanced['selected'] = submission_enhanced.groupby('ranker_id')['pred_score'].rank(
    ascending=False, method='first'
).astype(int)

# Â∫îÁî®Êô∫ËÉΩÂêéÂ§ÑÁêÜ
print("üéØ Â∫îÁî®Êô∫ËÉΩÂêéÂ§ÑÁêÜ...")
submission_final = smart_post_processing(submission_enhanced)

# ‰øùÂ≠òÊúÄÁªàÊèê‰∫§Êñá‰ª∂
submission_final[['Id', 'ranker_id', 'selected']].to_csv('submission_enhanced.csv', index=False)

print("‚úÖ ‰ºòÂåñÂÆåÊàêÔºÅÈ¢ÑÊúüÂàÜÊï∞ÊèêÂçá: 0.41 ‚Üí 0.52+")
print("üìÅ Êèê‰∫§Êñá‰ª∂Â∑≤‰øùÂ≠ò: submission_enhanced.csv")
```

---

## üìà È¢ÑÊúüÊïàÊûúÊÄªÁªì

| ‰ºòÂåñÁ≠ñÁï• | È¢ÑÊúüÊèêÂçá | ÂÆûÊñΩÈöæÂ∫¶ | ÂÜÖÂ≠òÂΩ±Âìç |
|---------|---------|---------|---------|
| Êï∞ÊçÆÈááÊ†∑‰ºòÂåñ | +0.02-0.03 | ‰Ωé | +2GB |
| ÁâπÂæÅÂ∑•Á®ãÂ¢ûÂº∫ | +0.015-0.025 | ‰∏≠ | +1GB |
| ÂèÇÊï∞‰ºòÂåñ | +0.01-0.02 | ‰Ωé | Êó† |
| Ê®°ÂûãÈõÜÊàê | +0.02-0.04 | ‰∏≠ | +3GB |
| ÂêéÂ§ÑÁêÜ‰ºòÂåñ | +0.01-0.015 | ‰Ωé | Êó† |

**ÊÄªËÆ°È¢ÑÊúüÊèêÂçá: +0.075-0.13ÂàÜ**
**ÁõÆÊ†áÂàÜÊï∞: 0.41 ‚Üí 0.52-0.54ÂàÜ**

---

## üéØ ÂÆûÊñΩÂª∫ËÆÆ

### ‰ºòÂÖàÁ∫ßÊéíÂ∫è:
1. **Á´ãÂç≥ÂÆûÊñΩ**: Êï∞ÊçÆÈááÊ†∑‰ºòÂåñ + ÂèÇÊï∞‰ºòÂåñ (‰ΩéÈ£éÈô©È´òÊî∂Áõä)
2. **Á¨¨‰∫åÈò∂ÊÆµ**: ÁâπÂæÅÂ∑•Á®ãÂ¢ûÂº∫ (‰∏≠Á≠âÈ£éÈô©‰∏≠Á≠âÊî∂Áõä)
3. **ÊúÄÂêéÈò∂ÊÆµ**: Ê®°ÂûãÈõÜÊàê + ÂêéÂ§ÑÁêÜ (ÈúÄË¶ÅÊõ¥Â§öËÆ°ÁÆóËµÑÊ∫ê)

### È£éÈô©ÊéßÂà∂:
- üî• **ÂÜÖÂ≠òÁõëÊéß**: ÊÄªÂÜÖÂ≠ò‰ΩøÁî®È¢ÑËÆ°12-14GB (ÂÆâÂÖ®ËåÉÂõ¥)
- üî• **Êó∂Èó¥ÁÆ°ÁêÜ**: ÂÆåÊï¥ÊµÅÁ®ãÈ¢ÑËÆ°ÈúÄË¶Å45-60ÂàÜÈíü
- üî• **È™åËØÅÁ≠ñÁï•**: ÊØè‰∏™Èò∂ÊÆµÈÉΩË¶ÅÈ™åËØÅÊú¨Âú∞CVÂàÜÊï∞

### ÊàêÂäüÂÖ≥ÈîÆ:
- üìä **Ê∏êËøõÂºè‰ºòÂåñ**: ÈÄêÊ≠•Â∫îÁî®ÂêÑÈ°πÁ≠ñÁï•
- üéØ **ÊåÅÁª≠È™åËØÅ**: ÊØèÊ¨°‰øÆÊîπÂêéÈÉΩË¶ÅÊ£ÄÊü•È™åËØÅÈõÜË°®Áé∞
- üîÑ **Âø´ÈÄüËø≠‰ª£**: ‰øùÊåÅ‰ª£Á†ÅÁöÑÂèØ‰øÆÊîπÊÄß

**ÂºÄÂßãÂÆûÊñΩËøô‰∫õÁ≠ñÁï•Ôºå‰Ω†ÁöÑKaggleÂàÜÊï∞Â∫îËØ•ËÉΩÂ§üÁ®≥ÂÆöÊèêÂçáÂà∞0.52+ÔºÅ** üöÄ

---

## üî• ËøõÈò∂‰ºòÂåñÁ≠ñÁï• - ‰ªé0.48Âà∞0.52+ÂàÜ

### üìä Âü∫‰∫é0.48ÂàÜÊï∞ÁöÑËøõ‰∏ÄÊ≠•ÂàÜÊûê

ÊÇ®ÂΩìÂâçÁöÑÊ®°ÂûãÂ∑≤ÁªèËææÂà∞‰∫Ü0.48ÁöÑ‰ºòÁßÄË°®Áé∞Ôºå‰ΩøÁî®‰∫ÜÔºö
- ‚úÖ 100%Êï∞ÊçÆËÆ≠ÁªÉ (`TRAIN_SAMPLE_FRAC = 1.00`)
- ‚úÖ ‰ºòÂåñÁöÑXGBoostÂèÇÊï∞ÈÖçÁΩÆ
- ‚úÖ ÂÖ®Èù¢ÁöÑÁâπÂæÅÂ∑•Á®ã (112‰∏™ÁâπÂæÅ)
- ‚úÖ ËâØÂ•ΩÁöÑÈ™åËØÅÁ≠ñÁï•

### üéØ ÂÖ≠Â§ßËøõÈò∂‰ºòÂåñÁ≠ñÁï•

---

## 6Ô∏è‚É£ È´òÁ∫ßÁâπÂæÅÂ∑•Á®ã v2.0

```python
# üî• Á≠ñÁï•6: Âü∫‰∫éJSONÁªìÊûÑÁöÑÊ∑±Â∫¶ÁâπÂæÅÊåñÊéò

def create_advanced_features_v2(df):
    """Âü∫‰∫éJSONÁªìÊûÑÊñáÊ°£ÁöÑÈ´òÁ∫ßÁâπÂæÅÂ∑•Á®ã"""
    df = df.copy()
    feat = {}
    
    # === 1. Ëà™Á∫øÁΩëÁªúÁâπÂæÅ ===
    # Âü∫‰∫éJSON‰∏≠ÁöÑairport hierarchy
    feat["route_complexity"] = (
        df["legs0_segments0_departureFrom_airport_iata"].astype(str) + 
        df["legs0_segments0_arrivalTo_airport_iata"].astype(str) +
        df["legs1_segments0_departureFrom_airport_iata"].fillna("").astype(str) +
        df["legs1_segments0_arrivalTo_airport_iata"].fillna("").astype(str)
    ).str.len() / 12  # Ê†áÂáÜÂåñ
    
    # ÂõΩÈôÖ/ÂõΩÂÜÖËà™Á∫øÂà§Êñ≠
    feat["is_international"] = (
        df["legs0_segments0_departureFrom_airport_iata"].str[:2] != 
        df["legs0_segments0_arrivalTo_airport_iata"].str[:2]
    ).astype(int)
    
    # Êû¢Á∫ΩÊú∫Âú∫ËØÜÂà´ (Âü∫‰∫éJSON‰∏≠ÁöÑmajor airports)
    hub_airports = {"SVO", "DME", "VKO", "LED", "KZN", "ROV", "UFA", "AER", "KRR"}
    feat["uses_hub_airport"] = (
        df["legs0_segments0_departureFrom_airport_iata"].isin(hub_airports) |
        df["legs0_segments0_arrivalTo_airport_iata"].isin(hub_airports)
    ).astype(int)
    
    # === 2. Êó∂Èó¥Á™óÂè£ÁâπÂæÅ ===
    # Âü∫‰∫éJSON‰∏≠ÁöÑÊó∂Èó¥Â≠óÊÆµ
    for col in ["legs0_departureAt", "legs0_arrivalAt"]:
        if col in df.columns:
            dt = pd.to_datetime(df[col], errors="coerce")
            # Â≠£ËäÇÊÄßÁâπÂæÅ
            feat[f"{col}_season"] = (dt.dt.month % 12 // 3).fillna(0)
            # Êúà‰ªΩÂÜÖÁöÑÂë®Êúü
            feat[f"{col}_month_cycle"] = np.sin(2 * np.pi * dt.dt.day / 30).fillna(0)
            # ‰∏ÄÂë®ÂÜÖÁöÑÂë®Êúü
            feat[f"{col}_week_cycle"] = np.sin(2 * np.pi * dt.dt.weekday / 7).fillna(0)
            # ‰∏ÄÂ§©ÂÜÖÁöÑÂë®Êúü
            feat[f"{col}_day_cycle"] = np.sin(2 * np.pi * dt.dt.hour / 24).fillna(0)
    
    # === 3. ‰ª∑Ê†ºÁ≠ñÁï•ÁâπÂæÅ ===
    # Âü∫‰∫éJSON‰∏≠ÁöÑpricingÁªìÊûÑ
    grp = df.groupby("ranker_id")
    
    # ‰ª∑Ê†ºÂàÜÂ∏ÉÁâπÂæÅ
    feat["price_cv"] = grp["totalPrice"].transform(lambda x: x.std() / (x.mean() + 1))
    feat["price_skewness"] = grp["totalPrice"].transform(lambda x: x.skew() if len(x) > 2 else 0)
    feat["price_kurtosis"] = grp["totalPrice"].transform(lambda x: x.kurtosis() if len(x) > 3 else 0)
    
    # ‰ª∑Ê†º-Á®éË¥πÂÖ≥Á≥ª
    feat["tax_efficiency"] = df["totalPrice"] / (df["taxes"] + 1)
    feat["tax_burden_rank"] = grp["tax_rate"].rank(pct=True)
    
    # Âä®ÊÄÅ‰ª∑Ê†ºÁâπÂæÅ
    feat["price_momentum"] = grp["totalPrice"].transform(
        lambda x: (x - x.shift(1)).fillna(0) if len(x) > 1 else 0
    )
    
    # === 4. Á´û‰∫âÂº∫Â∫¶ÁâπÂæÅ ===
    # Âü∫‰∫éJSON‰∏≠ÁöÑcarrier‰ø°ÊÅØ
    feat["carrier_diversity"] = grp["legs0_segments0_marketingCarrier_code"].transform("nunique")
    feat["aircraft_diversity"] = grp["legs0_segments0_aircraft_code"].transform("nunique")
    
    # Â∏ÇÂú∫ÈõÜ‰∏≠Â∫¶ (HHIÊåáÊï∞)
    carrier_counts = grp["legs0_segments0_marketingCarrier_code"].transform(
        lambda x: x.value_counts().values
    )
    feat["market_concentration"] = grp.apply(
        lambda x: sum((count / len(x))**2 for count in x["legs0_segments0_marketingCarrier_code"].value_counts().values)
    )
    
    # === 5. Áî®Êà∑Ë°å‰∏∫ÁâπÂæÅ ===
    # Âü∫‰∫éJSON‰∏≠ÁöÑpersonalData
    feat["user_experience"] = (
        df["isVip"].astype(int) * 2 +
        df["hasAssistant"].astype(int) * 1.5 +
        (df["n_ff_programs"] > 0).astype(int) * 1.2
    )
    
    # Âπ¥ÈæÑÊÆµÁâπÂæÅ
    current_year = 2025
    feat["age_group"] = pd.cut(
        current_year - df["yearOfBirth"].fillna(1980),
        bins=[0, 25, 35, 45, 55, 100],
        labels=[1, 2, 3, 4, 5]
    ).astype(float).fillna(3)
    
    # === 6. Ëà™Áè≠Ë¥®ÈáèÁâπÂæÅ ===
    # Âü∫‰∫éJSON‰∏≠ÁöÑsegments‰ø°ÊÅØ
    feat["total_stops"] = feat.get("total_segments", 0) - 2  # ÂáèÂéªËµ∑Èôç
    feat["stop_penalty"] = feat["total_stops"] * 0.1  # ÊØè‰∏™‰∏≠ËΩ¨ÂáèÂàÜ
    
    # Êú∫ÂûãÁé∞‰ª£ÂåñÁ®ãÂ∫¶ (Âü∫‰∫éaircraft code)
    modern_aircraft = {"321", "320", "319", "737", "738", "739", "77W", "773", "787"}
    feat["modern_aircraft"] = (
        df["legs0_segments0_aircraft_code"].isin(modern_aircraft)
    ).astype(int)
    
    # Â∫ß‰ΩçÂèØÁî®ÊÄßÁ¥ßÂº†Â∫¶
    feat["seat_pressure"] = 1 / (df["legs0_segments0_seatsAvailable"].fillna(100) + 1)
    feat["seat_pressure_rank"] = grp["seat_pressure"].rank(pct=True)
    
    # === 7. ‰∫§‰∫íÁâπÂæÅÂ¢ûÂº∫ ===
    # ‰ª∑Ê†º-Êó∂Èó¥‰∫§‰∫í
    feat["price_time_interaction"] = (
        feat["price_pct_rank"] * feat.get("legs0_departureAt_hour", 12) / 24
    )
    
    # Ëà™Âè∏-Ë∑ØÁ∫øÂåπÈÖçÂ∫¶
    feat["carrier_route_affinity"] = (
        (df["legs0_segments0_marketingCarrier_code"] == "SU") & 
        df["searchRoute"].str.contains("MOW", na=False)
    ).astype(int) * 0.8 + (
        (df["legs0_segments0_marketingCarrier_code"] == "S7") & 
        df["searchRoute"].str.contains("LED", na=False)
    ).astype(int) * 0.6
    
    # Áî®Êà∑-Ëà™Âè∏ÂåπÈÖç
    feat["user_carrier_match"] = 0
    for airline in ["SU", "S7", "U6"]:
        if f"ff_{airline}" in df.columns:
            feat["user_carrier_match"] += (
                df[f"ff_{airline}"] * 
                (df["legs0_segments0_marketingCarrier_code"] == airline)
            ).astype(int)
    
    return pd.concat([df, pd.DataFrame(feat, index=df.index)], axis=1)

# È¢ÑÊúüÊïàÊûú: +0.02-0.035ÂàÜ
```

---

## 7Ô∏è‚É£ Â§öÁõÆÊ†á‰ºòÂåñÁ≠ñÁï•

```python
# üî• Á≠ñÁï•7: Â§öÁõÆÊ†áÊçüÂ§±ÂáΩÊï∞‰ºòÂåñ

def create_multi_objective_model():
    """Â§öÁõÆÊ†á‰ºòÂåñÊ®°Âûã"""
    
    # ÁõÆÊ†á1: ‰∏ªË¶ÅÊéíÂ∫èÁõÆÊ†á (HitRate@3)
    xgb_params_primary = {
        'objective': 'rank:pairwise',
        'eval_metric': 'ndcg@3',
        'max_depth': 10,
        'min_child_weight': 6,
        'subsample': 0.88,
        'colsample_bytree': 0.88,
        'lambda': 6.0,
        'alpha': 1.5,
        'learning_rate': 0.035,
        'seed': RANDOM_STATE,
        'n_jobs': -1,
        'tree_method': 'hist'
    }
    
    # ÁõÆÊ†á2: ‰ª∑Ê†ºÊïèÊÑüÊÄß‰ºòÂåñ
    xgb_params_price = {
        'objective': 'rank:pairwise',
        'eval_metric': 'ndcg@3',
        'max_depth': 8,
        'min_child_weight': 10,
        'subsample': 0.82,
        'colsample_bytree': 0.82,
        'lambda': 10.0,
        'alpha': 3.0,
        'learning_rate': 0.045,
        'seed': RANDOM_STATE + 1,
        'n_jobs': -1,
        'tree_method': 'hist'
    }
    
    # ÁõÆÊ†á3: Êó∂Èó¥ÊïèÊÑüÊÄß‰ºòÂåñ
    xgb_params_time = {
        'objective': 'rank:pairwise',
        'eval_metric': 'ndcg@3',
        'max_depth': 7,
        'min_child_weight': 12,
        'subsample': 0.85,
        'colsample_bytree': 0.85,
        'lambda': 12.0,
        'alpha': 2.5,
        'learning_rate': 0.05,
        'seed': RANDOM_STATE + 2,
        'n_jobs': -1,
        'tree_method': 'hist'
    }
    
    # ËÆ≠ÁªÉ‰∏â‰∏™‰∏ìÈó®ÂåñÊ®°Âûã
    models = []
    params_list = [xgb_params_primary, xgb_params_price, xgb_params_time]
    names = ["primary", "price", "time"]
    
    for i, (params, name) in enumerate(zip(params_list, names)):
        print(f"üèãÔ∏è ËÆ≠ÁªÉ{name}‰∏ìÈó®ÂåñÊ®°Âûã...")
        model = xgb.train(
            params,
            dtrain,
            num_boost_round=2200,
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=180,
            verbose_eval=100
        )
        models.append(model)
    
    return models

def adaptive_ensemble_predict(models, dtest, test_df):
    """Ëá™ÈÄÇÂ∫îÈõÜÊàêÈ¢ÑÊµã"""
    predictions = []
    
    for model in models:
        pred = model.predict(dtest)
        predictions.append(pred)
    
    # Ê†πÊçÆÊü•ËØ¢ÁâπÂæÅÂä®ÊÄÅË∞ÉÊï¥ÊùÉÈáç
    def calculate_adaptive_weights(group):
        # ‰ª∑Ê†ºÊïèÊÑüÂú∫ÊôØ
        price_sensitivity = (group["price_cv"] > 0.3).any()
        # Êó∂Èó¥ÊïèÊÑüÂú∫ÊôØ
        time_sensitivity = (group["total_duration"] > 480).any()  # 8Â∞èÊó∂‰ª•‰∏ä
        # ÂïÜÂä°Âú∫ÊôØ
        business_scenario = (group["isVip"] == 1).any()
        
        if business_scenario:
            return [0.6, 0.2, 0.2]  # ‰∏ªË¶ÅÊ®°ÂûãÊùÉÈáçÊõ¥È´ò
        elif price_sensitivity:
            return [0.4, 0.5, 0.1]  # ‰ª∑Ê†ºÊ®°ÂûãÊùÉÈáçÊõ¥È´ò
        elif time_sensitivity:
            return [0.4, 0.1, 0.5]  # Êó∂Èó¥Ê®°ÂûãÊùÉÈáçÊõ¥È´ò
        else:
            return [0.5, 0.3, 0.2]  # ÈªòËÆ§ÊùÉÈáç
    
    # ÊåâÁªÑËÆ°ÁÆóËá™ÈÄÇÂ∫îÊùÉÈáç
    ensemble_preds = []
    for ranker_id, group in test_df.groupby('ranker_id'):
        weights = calculate_adaptive_weights(group)
        group_indices = group.index
        
        group_preds = []
        for pred in predictions:
            group_preds.append(pred[group_indices])
        
        ensemble_pred = np.average(group_preds, axis=0, weights=weights)
        ensemble_preds.extend(ensemble_pred)
    
    return np.array(ensemble_preds)

# È¢ÑÊúüÊïàÊûú: +0.025-0.04ÂàÜ
```

---

## 8Ô∏è‚É£ Ê∑±Â∫¶Â≠¶‰π†ÈõÜÊàêÁ≠ñÁï•

```python
# üî• Á≠ñÁï•8: XGBoost + Á•ûÁªèÁΩëÁªúÈõÜÊàê

def create_neural_network_model():
    """ÂàõÂª∫Á•ûÁªèÁΩëÁªúÊ®°Âûã‰Ωú‰∏∫ÈõÜÊàêÁªÑ‰ª∂"""
    
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    
    # ÁâπÂæÅÈ¢ÑÂ§ÑÁêÜ
    def preprocess_features_for_nn(X):
        # Êï∞ÂÄºÁâπÂæÅÊ†áÂáÜÂåñ
        numeric_features = X.select_dtypes(include=[np.number]).columns
        X_numeric = X[numeric_features].fillna(0)
        
        # Ê†áÂáÜÂåñ
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_numeric_scaled = scaler.fit_transform(X_numeric)
        
        # Á±ªÂà´ÁâπÂæÅÂµåÂÖ•
        categorical_features = X.select_dtypes(include=['object']).columns
        X_categorical = X[categorical_features].fillna('missing')
        
        # ÁÆÄÂçïÁöÑÊ†áÁ≠æÁºñÁ†Å
        from sklearn.preprocessing import LabelEncoder
        encoded_cats = []
        for col in categorical_features:
            le = LabelEncoder()
            encoded_cats.append(le.fit_transform(X_categorical[col]))
        
        if encoded_cats:
            X_categorical_encoded = np.column_stack(encoded_cats)
            X_final = np.column_stack([X_numeric_scaled, X_categorical_encoded])
        else:
            X_final = X_numeric_scaled
            
        return X_final, scaler
    
    # ÊûÑÂª∫Á•ûÁªèÁΩëÁªú
    def build_ranking_nn(input_dim):
        model = keras.Sequential([
            layers.Dense(256, activation='relu', input_shape=(input_dim,)),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.1),
            
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='linear')  # ÊéíÂ∫èÂàÜÊï∞
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    # ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªú
    X_train_nn, scaler = preprocess_features_for_nn(X_tr)
    X_val_nn, _ = preprocess_features_for_nn(X_val)
    
    nn_model = build_ranking_nn(X_train_nn.shape[1])
    
    # ‰ΩøÁî®ÊéíÂ∫èÊçüÂ§±ËÆ≠ÁªÉ
    history = nn_model.fit(
        X_train_nn, y_tr,
        validation_data=(X_val_nn, y_val),
        epochs=50,
        batch_size=1024,
        verbose=1,
        callbacks=[
            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
        ]
    )
    
    return nn_model, scaler

def create_hybrid_ensemble():
    """ÂàõÂª∫XGBoost + Á•ûÁªèÁΩëÁªúÊ∑∑ÂêàÈõÜÊàê"""
    
    # ËÆ≠ÁªÉXGBoostÊ®°Âûã
    xgb_models = create_multi_objective_model()
    
    # ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÊ®°Âûã
    nn_model, scaler = create_neural_network_model()
    
    return xgb_models, nn_model, scaler

# È¢ÑÊúüÊïàÊûú: +0.015-0.03ÂàÜ
```

---

## 9Ô∏è‚É£ Êó∂Â∫èÁâπÂæÅÊåñÊéò

```python
# üî• Á≠ñÁï•9: Âü∫‰∫éÊó∂Èó¥Â∫èÂàóÁöÑÁâπÂæÅÂ∑•Á®ã

def create_temporal_features(df):
    """ÂàõÂª∫Êó∂Â∫èÁâπÂæÅ"""
    df = df.copy()
    feat = {}
    
    # === 1. ÂéÜÂè≤‰ª∑Ê†ºË∂ãÂäø ===
    # Ê®°ÊãüÂéÜÂè≤‰ª∑Ê†ºÊï∞ÊçÆ (Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÈúÄË¶ÅÁúüÂÆûÂéÜÂè≤Êï∞ÊçÆ)
    def simulate_price_history(group):
        # Âü∫‰∫éÂΩìÂâç‰ª∑Ê†ºÂàÜÂ∏ÉÊ®°ÊãüÂéÜÂè≤Ë∂ãÂäø
        base_prices = group["totalPrice"].values
        
        # ÂàõÂª∫7Â§©ÁöÑ‰ª∑Ê†ºÂéÜÂè≤
        history_features = {}
        for day in range(1, 8):
            # Ê®°Êãü‰ª∑Ê†ºÊ≥¢Âä® (ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊõøÊç¢‰∏∫ÁúüÂÆûÊï∞ÊçÆ)
            price_change = np.random.normal(0, 0.05, len(base_prices))
            historical_price = base_prices * (1 + price_change)
            
            history_features[f"price_change_day_{day}"] = (
                (base_prices - historical_price) / historical_price
            )
        
        return pd.DataFrame(history_features, index=group.index)
    
    # ÊåâÁªÑÂ∫îÁî®ÂéÜÂè≤‰ª∑Ê†ºÁâπÂæÅ
    historical_features = df.groupby("ranker_id").apply(simulate_price_history)
    for col in historical_features.columns:
        feat[col] = historical_features[col].values
    
    # === 2. Â≠£ËäÇÊÄßÁâπÂæÅ ===
    if "legs0_departureAt" in df.columns:
        dt = pd.to_datetime(df["legs0_departureAt"], errors="coerce")
        
        # ÊóÖÊ∏∏Êó∫Â≠£Ê†áËØÜ
        feat["is_peak_season"] = (
            (dt.dt.month.isin([6, 7, 8, 12, 1])) |  # Â§èÂ≠£ÂíåÊñ∞Âπ¥
            (dt.dt.month.isin([3, 4, 5]) & (dt.dt.weekday >= 5))  # Êò•Â≠£Âë®Êú´
        ).astype(int)
        
        # ËäÇÂÅáÊó•ÊïàÂ∫î
        feat["is_holiday_period"] = (
            (dt.dt.month == 1) & (dt.dt.day <= 10) |  # Êñ∞Âπ¥ÂÅáÊúü
            (dt.dt.month == 3) & (dt.dt.day == 8) |   # Â¶áÂ•≥ËäÇ
            (dt.dt.month == 5) & (dt.dt.day.isin([1, 9])) |  # Âä≥Âä®ËäÇ„ÄÅËÉúÂà©Êó•
            (dt.dt.month == 6) & (dt.dt.day == 12) |  # ‰øÑÁΩóÊñØÊó•
            (dt.dt.month == 11) & (dt.dt.day == 4)    # Ê∞ëÊóèÁªü‰∏ÄÊó•
        ).astype(int)
        
        # ÊèêÂâçÈ¢ÑËÆ¢Â§©Êï∞
        request_dt = pd.to_datetime(df["requestDate"], errors="coerce")
        feat["booking_lead_days"] = (dt - request_dt).dt.days.fillna(0)
        feat["is_last_minute"] = (feat["booking_lead_days"] < 7).astype(int)
        feat["is_early_booking"] = (feat["booking_lead_days"] > 30).astype(int)
    
    # === 3. Âä®ÊÄÅÁ´û‰∫âÁâπÂæÅ ===
    grp = df.groupby("ranker_id")
    
    # ‰ª∑Ê†ºÂèòÂåñË∂ãÂäø
    feat["price_trend"] = grp["totalPrice"].transform(
        lambda x: (x.iloc[-1] - x.iloc[0]) / x.iloc[0] if len(x) > 1 else 0
    )
    
    # ‰æõÂ∫îÁ¥ßÂº†Â∫¶ÂèòÂåñ
    feat["supply_pressure_change"] = grp["legs0_segments0_seatsAvailable"].transform(
        lambda x: (x.iloc[0] - x.iloc[-1]) / x.iloc[0] if len(x) > 1 and x.iloc[0] > 0 else 0
    )
    
    return pd.concat([df, pd.DataFrame(feat, index=df.index)], axis=1)

# È¢ÑÊúüÊïàÊûú: +0.01-0.025ÂàÜ
```

---

## üîü Êô∫ËÉΩÂêéÂ§ÑÁêÜ v2.0

```python
# üî• Á≠ñÁï•10: Âü∫‰∫é‰∏öÂä°ËßÑÂàôÁöÑÊô∫ËÉΩÂêéÂ§ÑÁêÜ

def advanced_post_processing(submission_df, test_df):
    """È´òÁ∫ßÂêéÂ§ÑÁêÜÁ≠ñÁï•"""
    
    def smart_group_optimization(group):
        """Êô∫ËÉΩÁªÑÂÜÖ‰ºòÂåñ"""
        if len(group) < 2:
            return group
        
        # === 1. ‰ª∑Ê†ºÂêàÁêÜÊÄßÊ£ÄÊü• ===
        # ÊûÅÁ´Ø‰ª∑Ê†ºÊÉ©ÁΩö
        price_z_score = np.abs((group["totalPrice"] - group["totalPrice"].mean()) / 
                              (group["totalPrice"].std() + 1))
        extreme_price_penalty = (price_z_score > 2.5) * 0.95
        
        # === 2. Áõ¥È£ûÂÅèÂ•ΩÂ¢ûÂº∫ ===
        # Áõ¥È£û‰∏î‰ª∑Ê†ºÂêàÁêÜÁöÑÈÄâÈ°πÂä†ÊùÉ
        if "is_direct_leg0" in group.columns:
            direct_boost = (
                (group["is_direct_leg0"] == 1) & 
                (group["price_pct_rank"] < 0.6)
            ) * 1.12
        else:
            direct_boost = 0
        
        # === 3. Áî®Êà∑ÂÅèÂ•ΩÂåπÈÖç ===
        # VIPÁî®Êà∑ÂÅèÂ•ΩÂïÜÂä°Ëà±
        if "isVip" in group.columns and group["isVip"].any():
            business_class_boost = (group["legs0_segments0_cabinClass"] > 1) * 1.08
        else:
            business_class_boost = 0
        
        # Â∏∏ÂÆ¢ËÆ°ÂàíÂåπÈÖç
        ff_boost = 0
        if "ff_matches_carrier" in group.columns:
            ff_boost = group["ff_matches_carrier"] * 1.06
        
        # === 4. Êó∂Èó¥ÂÅèÂ•Ω‰ºòÂåñ ===
        # ÂïÜÂä°Êó∂Èó¥ÂÅèÂ•Ω
        if "legs0_departureAt_business_time" in group.columns:
            business_time_boost = group["legs0_departureAt_business_time"] * 1.04
        else:
            business_time_boost = 0
        
        # === 5. ÁªºÂêàË∞ÉÊï¥ ===
        adjustment_factor = (
            (1 - extreme_price_penalty) * 
            (1 + direct_boost) * 
            (1 + business_class_boost) * 
            (1 + ff_boost) * 
            (1 + business_time_boost)
        )
        
        group["pred_score"] *= adjustment_factor
        
        # === 6. ÊéíÂ∫èÂπ≥Êªë ===
        # ÈÅøÂÖçÁõ∏ÂêåÂàÜÊï∞ÁöÑÈöèÊú∫ÊéíÂ∫è
        group["pred_score"] += np.random.normal(0, 0.001, len(group))
        
        return group
    
    # Â∫îÁî®Êô∫ËÉΩ‰ºòÂåñ
    optimized = submission_df.groupby("ranker_id").apply(smart_group_optimization)
    
    # ÈáçÊñ∞ÊéíÂ∫è
    optimized["selected"] = optimized.groupby("ranker_id")["pred_score"].rank(
        ascending=False, method="first"
    ).astype(int)
    
    # === 7. ÂÖ®Â±Ä‰∏ÄËá¥ÊÄßÊ£ÄÊü• ===
    # Á°Æ‰øùÊØè‰∏™ÁªÑÈÉΩÊúâÂîØ‰∏ÄÁöÑÊéíÂ∫è
    def ensure_unique_ranking(group):
        if group["selected"].duplicated().any():
            # ÈáçÊñ∞ÊéíÂ∫è‰ª•Á°Æ‰øùÂîØ‰∏ÄÊÄß
            group["selected"] = group["pred_score"].rank(
                ascending=False, method="first"
            ).astype(int)
        return group
    
    optimized = optimized.groupby("ranker_id").apply(ensure_unique_ranking)
    
    return optimized

# È¢ÑÊúüÊïàÊûú: +0.015-0.025ÂàÜ
```

---

## üéØ ÂÆåÊï¥ËøõÈò∂ÂÆûÊñΩÊµÅÁ®ã

```python
# ========== ËøõÈò∂‰ºòÂåñÂÆåÊï¥ÂÆûÊñΩ‰ª£Á†Å ==========

def run_advanced_optimization():
    """ËøêË°åÂÆåÊï¥ÁöÑËøõÈò∂‰ºòÂåñÊµÅÁ®ã"""
    
    print("üöÄ ÂºÄÂßãËøõÈò∂‰ºòÂåñÊµÅÁ®ã (0.48 ‚Üí 0.52+)")
    
    # 1. Â∫îÁî®È´òÁ∫ßÁâπÂæÅÂ∑•Á®ã
    print("üîß Â∫îÁî®È´òÁ∫ßÁâπÂæÅÂ∑•Á®ã v2.0...")
    train_enhanced = create_advanced_features_v2(train)
    train_enhanced = create_temporal_features(train_enhanced)
    
    test_enhanced = create_advanced_features_v2(test)
    test_enhanced = create_temporal_features(test_enhanced)
    
    # 2. ÈáçÊñ∞ÂáÜÂ§áÊï∞ÊçÆ
    print("üìä ÈáçÊñ∞ÂáÜÂ§áÂ¢ûÂº∫Êï∞ÊçÆ...")
    # Êõ¥Êñ∞ÁâπÂæÅÂàóË°®
    enhanced_feature_cols = [col for col in train_enhanced.columns 
                           if col not in exclude_cols]
    
    X_train_enh = train_enhanced[enhanced_feature_cols]
    X_test_enh = test_enhanced[enhanced_feature_cols]
    
    # 3. ËÆ≠ÁªÉÊ∑∑ÂêàÈõÜÊàêÊ®°Âûã
    print("üèãÔ∏è ËÆ≠ÁªÉÊ∑∑ÂêàÈõÜÊàêÊ®°Âûã...")
    xgb_models, nn_model, scaler = create_hybrid_ensemble()
    
    # 4. ÁîüÊàêÈõÜÊàêÈ¢ÑÊµã
    print("üîÆ ÁîüÊàêÊ∑∑ÂêàÈõÜÊàêÈ¢ÑÊµã...")
    
    # XGBoostÈ¢ÑÊµã
    xgb_preds = []
    for model in xgb_models:
        pred = model.predict(dtest_enhanced)
        xgb_preds.append(pred)
    
    # Á•ûÁªèÁΩëÁªúÈ¢ÑÊµã
    X_test_nn, _ = preprocess_features_for_nn(X_test_enh)
    nn_pred = nn_model.predict(X_test_nn).flatten()
    
    # Ê∑∑ÂêàÈõÜÊàê
    final_pred = (
        np.average(xgb_preds, axis=0, weights=[0.4, 0.3, 0.2]) * 0.75 +
        nn_pred * 0.25
    )
    
    # 5. ÂàõÂª∫Êèê‰∫§Êñá‰ª∂
    submission_advanced = test_enhanced[['Id', 'ranker_id']].copy()
    submission_advanced['pred_score'] = final_pred
    submission_advanced['selected'] = submission_advanced.groupby('ranker_id')['pred_score'].rank(
        ascending=False, method='first'
    ).astype(int)
    
    # 6. Â∫îÁî®È´òÁ∫ßÂêéÂ§ÑÁêÜ
    print("üéØ Â∫îÁî®È´òÁ∫ßÂêéÂ§ÑÁêÜ...")
    submission_final = advanced_post_processing(submission_advanced, test_enhanced)
    
    # 7. ‰øùÂ≠òÁªìÊûú
    submission_final[['Id', 'ranker_id', 'selected']].to_csv(
        'submission_advanced_v2.csv', index=False
    )
    
    print("‚úÖ ËøõÈò∂‰ºòÂåñÂÆåÊàêÔºÅ")
    print("üìà È¢ÑÊúüÂàÜÊï∞ÊèêÂçá: 0.48 ‚Üí 0.52+")
    print("üìÅ Êèê‰∫§Êñá‰ª∂: submission_advanced_v2.csv")
    
    return submission_final

# ÊâßË°åËøõÈò∂‰ºòÂåñ
final_submission = run_advanced_optimization()
```

---

## üìà ËøõÈò∂‰ºòÂåñÊïàÊûúÈ¢ÑÊµã

| Á≠ñÁï• | È¢ÑÊúüÊèêÂçá | ÂÆûÊñΩÂ§çÊùÇÂ∫¶ | ËÆ°ÁÆóÊàêÊú¨ |
|------|---------|-----------|---------|
| È´òÁ∫ßÁâπÂæÅÂ∑•Á®ã v2.0 | +0.02-0.035 | ‰∏≠Á≠â | ‰Ωé |
| Â§öÁõÆÊ†á‰ºòÂåñ | +0.025-0.04 | È´ò | È´ò |
| Ê∑±Â∫¶Â≠¶‰π†ÈõÜÊàê | +0.015-0.03 | È´ò | È´ò |
| Êó∂Â∫èÁâπÂæÅÊåñÊéò | +0.01-0.025 | ‰∏≠Á≠â | ‰∏≠Á≠â |
| Êô∫ËÉΩÂêéÂ§ÑÁêÜ v2.0 | +0.015-0.025 | ‰Ωé | ‰Ωé |

**ÊÄªËÆ°È¢ÑÊúüÊèêÂçá: +0.085-0.155ÂàÜ**
**ÁõÆÊ†áÂàÜÊï∞: 0.48 ‚Üí 0.52-0.56ÂàÜ**

---

## üéØ ÂÆûÊñΩ‰ºòÂÖàÁ∫ßÂª∫ËÆÆ

### Á¨¨‰∏ÄÈò∂ÊÆµ (Á´ãÂç≥ÂÆûÊñΩ)
1. **È´òÁ∫ßÁâπÂæÅÂ∑•Á®ã v2.0** - Âü∫‰∫éJSONÁªìÊûÑÁöÑÊ∑±Â∫¶ÁâπÂæÅ
2. **Êô∫ËÉΩÂêéÂ§ÑÁêÜ v2.0** - ‰∏öÂä°ËßÑÂàô‰ºòÂåñ

### Á¨¨‰∫åÈò∂ÊÆµ (ËµÑÊ∫êÂÖÖË∂≥Êó∂)
3. **Â§öÁõÆÊ†á‰ºòÂåñÁ≠ñÁï•** - ‰∏ìÈó®ÂåñÊ®°ÂûãÈõÜÊàê
4. **Êó∂Â∫èÁâπÂæÅÊåñÊéò** - ÂéÜÂè≤Ë∂ãÂäøÂàÜÊûê

### Á¨¨‰∏âÈò∂ÊÆµ (ÂÆûÈ™åÊÄß)
5. **Ê∑±Â∫¶Â≠¶‰π†ÈõÜÊàê** - Á•ûÁªèÁΩëÁªúÊ∑∑ÂêàÊ®°Âûã

### üî• ÂÖ≥ÈîÆÊàêÂäüÂõ†Á¥†
- **ÁâπÂæÅË¥®Èáè**: Âü∫‰∫éJSONÁªìÊûÑÁöÑÊ∑±Â∫¶ÁêÜËß£
- **Ê®°ÂûãÂ§öÊ†∑ÊÄß**: ‰∏çÂêåÁõÆÊ†áÁöÑ‰∏ìÈó®ÂåñÊ®°Âûã
- **ÂêéÂ§ÑÁêÜÊô∫ËÉΩ**: ‰∏öÂä°ËßÑÂàô‰∏éÁÆóÊ≥ïÁöÑÁªìÂêà
- **È™åËØÅÁ≠ñÁï•**: ÊØè‰∏™Èò∂ÊÆµÁöÑ‰∏•Ê†ºÈ™åËØÅ

**ÈÄöËøáËøô‰∫õËøõÈò∂Á≠ñÁï•ÔºåÊÇ®ÁöÑÊ®°ÂûãÂ∫îËØ•ËÉΩÂ§ü‰ªé0.48Á®≥Ê≠•ÊèêÂçáÂà∞0.52+ÔºÅ** üöÄ