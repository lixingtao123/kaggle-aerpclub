# %% [markdown]
# # AeroClub RecSys 2025 - æ”¹è¿›çš„XGBoostæ’åºæ¨¡å‹
# 
# è¿™ä¸ªé¡¹ç›®æ˜¯å¯¹åŸå§‹åŸºçº¿æ¨¡å‹çš„å…¨é¢æ”¹è¿›ï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
# - å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹
# - æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
# - é”™è¯¯å¤„ç†å’Œå†…å­˜ä¼˜åŒ–
# - è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š

# %% [markdown]
# ## 1. å¯¼å…¥å¿…è¦çš„åº“

# %%
"""
AeroClub RecSys 2025 - æ”¹è¿›çš„XGBoostæ’åºæ¨¡å‹

é¡¹ç›®ç›®æ ‡ï¼š
1. æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šï¼Œæé«˜ä»£ç å¯è¯»æ€§
2. ä¿®å¤åŸå§‹ä»£ç ä¸­çš„æ½œåœ¨é—®é¢˜
3. ä¸ºåç»­çš„ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹ä¼˜åŒ–æ‰“ä¸‹åŸºç¡€
4. å®ç°å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import log_loss
import matplotlib.pyplot as plt
import xgboost as xgb
from typing import Tuple, List, Dict, Optional
import warnings
warnings.filterwarnings('ignore')

# %% [markdown]
# ## 2. å…¨å±€é…ç½®

# %%
# å…¨å±€å‚æ•°é…ç½®
TRAIN_SAMPLE_FRAC = 0.5  # è®­ç»ƒæ•°æ®é‡‡æ ·æ¯”ä¾‹ï¼Œç”¨äºå¿«é€Ÿè¿­ä»£
RANDOM_STATE = 42        # éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°

# è®¾ç½®éšæœºç§å­
np.random.seed(RANDOM_STATE)

# æ˜¾ç¤ºè®¾ç½®ï¼Œä¾¿äºæ•°æ®æŸ¥çœ‹
pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 100)

print("å…¨å±€é…ç½®å®Œæˆ")
print(f"è®­ç»ƒæ•°æ®é‡‡æ ·æ¯”ä¾‹: {TRAIN_SAMPLE_FRAC}")
print(f"éšæœºç§å­: {RANDOM_STATE}")

# %% [markdown]
# ## 3. èˆªç­æ’åºæ¨¡å‹ç±»å®šä¹‰

# %%
class FlightRankingModel:
    """
    èˆªç­æ¨èæ’åºæ¨¡å‹
    ä½¿ç”¨XGBoostå®ç°çš„æ’åºå­¦ä¹ æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºèˆªç­é€‰æ‹©é¢„æµ‹
    """
    
    def __init__(self, train_sample_frac: float = 0.5, random_state: int = 42, 
                 chunk_size: int = 100000, use_chunked_loading: bool = False):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            train_sample_frac: è®­ç»ƒæ•°æ®é‡‡æ ·æ¯”ä¾‹
            random_state: éšæœºç§å­
            chunk_size: åˆ†å—å¤§å°ï¼Œç”¨äºå¤§æ•°æ®é›†å¤„ç†
            use_chunked_loading: æ˜¯å¦ä½¿ç”¨åˆ†å—åŠ è½½
        """
        self.train_sample_frac = train_sample_frac
        self.random_state = random_state
        self.chunk_size = chunk_size
        self.use_chunked_loading = use_chunked_loading
        self.model = None
        self.feature_cols = []
        self.cat_features = []
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(random_state)
        
        # å…¨å±€é…ç½®
        pd.set_option('display.max_columns', 50)
        pd.set_option('display.max_rows', 20)
        
        print(f"FlightRankingModel åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è®­ç»ƒé‡‡æ ·æ¯”ä¾‹: {train_sample_frac}")
        print(f"   - éšæœºç§å­: {random_state}")
        print(f"   - åˆ†å—å¤§å°: {chunk_size:,}")
        print(f"   - åˆ†å—åŠ è½½: {'å¯ç”¨' if use_chunked_loading else 'ç¦ç”¨'}")

# %% [markdown]
# ## 4. æ•°æ®åŠ è½½åŠŸèƒ½

# %%
    def load_data_chunked(self, file_path: str) -> pd.DataFrame:
        """
        åˆ†å—åŠ è½½å¤§å‹parquetæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        print(f"å¼€å§‹åˆ†å—åŠ è½½æ•°æ®: {file_path}")
        chunks = []
        
        try:
            # ä½¿ç”¨pandasåˆ†å—è¯»å–
            chunk_iter = pd.read_parquet(file_path, chunksize=self.chunk_size)
            
            for i, chunk in enumerate(chunk_iter):
                print(f"  å¤„ç†åˆ†å— {i+1}: {len(chunk):,} è¡Œ")
                chunks.append(chunk)
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if (i + 1) % 10 == 0:
                    print(f"  å·²å¤„ç† {i+1} ä¸ªåˆ†å—ï¼Œæ­£åœ¨åˆå¹¶...")
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å†…å­˜æ¸…ç†é€»è¾‘
                    import gc
                    gc.collect()
            
            print(f"åˆå¹¶ {len(chunks)} ä¸ªåˆ†å—...")
            df = pd.concat(chunks, ignore_index=True)
            print(f"åˆ†å—åŠ è½½å®Œæˆ: {len(df):,} è¡Œ")
            
            return df
            
        except Exception as e:
            print(f"åˆ†å—åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°å¸¸è§„åŠ è½½
            print("å›é€€åˆ°å¸¸è§„åŠ è½½æ–¹å¼...")
            return pd.read_parquet(file_path)

    def load_data(self, train_path: str = 'train.parquet', test_path: str = 'test.parquet') -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        
        Args:
            train_path: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            test_path: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
        """
        print("å¼€å§‹åŠ è½½æ•°æ®...")
        
        try:
            if self.use_chunked_loading:
                # åˆ†å—åŠ è½½
                train_data = self.load_data_chunked(train_path)
                test_data = self.load_data_chunked(test_path)
            else:
                # å¸¸è§„åŠ è½½
                print(f"åŠ è½½è®­ç»ƒæ•°æ®: {train_path}")
                train_data = pd.read_parquet(train_path)
                print(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_path}")
                test_data = pd.read_parquet(test_path)
            
            print(f"æ•°æ®åŠ è½½å®Œæˆ")
            print(f"  è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape}")
            print(f"  æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
            print(f"  è®­ç»ƒé›†ä¸­å”¯ä¸€çš„ranker_idæ•°é‡: {train_data['ranker_id'].nunique():,}")
            print(f"  ç›®æ ‡å˜é‡(selected)çš„å¹³å‡å€¼: {train_data['selected'].mean():.3f}")
            
            return train_data, test_data
            
        except FileNotFoundError as e:
            print(f"æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯: {e}")
            print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")
            return pd.DataFrame(), pd.DataFrame()
        
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å‡ºé”™: {e}")
            return pd.DataFrame(), pd.DataFrame()

# %% [markdown]
# ## 5. æ•°æ®é‡‡æ ·åŠŸèƒ½

# %%
    def sample_data(self, train_df: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œé‡‡æ ·ï¼ˆæŒ‰ranker_idåˆ†ç»„é‡‡æ ·ï¼‰
        
        Args:
            train_df: åŸå§‹è®­ç»ƒæ•°æ®
            
        Returns:
            pd.DataFrame: é‡‡æ ·åçš„è®­ç»ƒæ•°æ®
        """
        if self.train_sample_frac >= 1.0:
            print("ğŸ”„ ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼Œæ— éœ€é‡‡æ ·")
            return train_df
        
        print(f"å¼€å§‹æ•°æ®é‡‡æ ·ï¼ˆé‡‡æ ·æ¯”ä¾‹: {self.train_sample_frac}ï¼‰")
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„ranker_id
        unique_rankers = train_df['ranker_id'].unique()
        n_sample = int(len(unique_rankers) * self.train_sample_frac)
        
        # éšæœºé€‰æ‹©ranker_idè¿›è¡Œé‡‡æ ·
        # è¿™æ ·åšå¯ä»¥ç¡®ä¿åŒä¸€ä¸ªranker_idä¸‹çš„æ‰€æœ‰é€‰é¡¹è¦ä¹ˆå…¨éƒ¨åŒ…å«ï¼Œè¦ä¹ˆå…¨éƒ¨æ’é™¤ï¼Œé¿å…æ•°æ®æ³„éœ²
        sampled_rankers = np.random.RandomState(self.random_state).choice(
            unique_rankers, size=n_sample, replace=False
        )
        
        # æ ¹æ®é€‰ä¸­çš„ranker_idè¿‡æ»¤æ•°æ®
        sampled_df = train_df[train_df['ranker_id'].isin(sampled_rankers)]
        
        print(f"æ•°æ®é‡‡æ ·å®Œæˆ")
        print(f"   - é‡‡æ ·å‰: {len(train_df):,} è¡Œï¼Œ{train_df['ranker_id'].nunique():,} ç»„")
        print(f"   - é‡‡æ ·å: {len(sampled_df):,} è¡Œï¼Œ{sampled_df['ranker_id'].nunique():,} ç»„")
        
        return sampled_df

# %% [markdown]
# ## 6. æ—¶é—´è½¬æ¢è¾…åŠ©å‡½æ•°

# %%
    def hms_to_minutes(self, s: pd.Series) -> np.ndarray:
        """
        å°† 'HH:MM:SS' æ ¼å¼çš„æ—¶é—´è½¬æ¢ä¸ºåˆ†é’Ÿæ•°ï¼ˆå¿½ç•¥ç§’ï¼‰
        
        Args:
            s: åŒ…å«æ—¶é—´å­—ç¬¦ä¸²çš„Series
            
        Returns:
            np.ndarray: è½¬æ¢åçš„åˆ†é’Ÿæ•°æ•°ç»„
        """
        mask = s.notna()
        out = np.zeros(len(s), dtype=float)
        if mask.any():
            parts = s[mask].astype(str).str.split(':', expand=True)
            out[mask] = (
                pd.to_numeric(parts[0], errors="coerce").fillna(0) * 60
                + pd.to_numeric(parts[1], errors="coerce").fillna(0)
            )
        return out

# %% [markdown]
# ## 7. ç‰¹å¾å·¥ç¨‹ä¸»å‡½æ•°

# %%
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºç‰¹å¾å·¥ç¨‹
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            åŒ…å«æ–°ç‰¹å¾çš„æ•°æ®
        """
        print("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œä½¿ç”¨åˆ†å—å¤„ç†
        if len(df) > self.chunk_size * 2:
            print(f"æ•°æ®é‡è¾ƒå¤§ ({len(df):,} è¡Œ)ï¼Œä½¿ç”¨åˆ†å—å¤„ç†...")
            return self.process_data_in_chunks(df, self._create_features_single_chunk)
        else:
            return self._create_features_single_chunk(df)
    
    def _create_features_single_chunk(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å•ä¸ªåˆ†å—çš„ç‰¹å¾å·¥ç¨‹å¤„ç†
        
        Args:
            df: è¾“å…¥æ•°æ®å—
            
        Returns:
            åŒ…å«æ–°ç‰¹å¾çš„æ•°æ®å—
        """
        df = df.copy()
        
        # å¤„ç†æ—¶é—´æ ¼å¼æ•°æ®
        dur_cols = (
            ["legs0_duration", "legs1_duration"]
            + [f"legs{l}_segments{s}_duration" for l in (0, 1) for s in (0, 1)]
        )
        for col in dur_cols:
            if col in df.columns:
                df[col] = self.hms_to_minutes(df[col])

        # ç‰¹å¾å®¹å™¨
        feat = {}

        # ä»·æ ¼ç‰¹å¾
        print("   åˆ›å»ºä»·æ ¼ç‰¹å¾...")
        feat["price_per_tax"] = df["totalPrice"] / (df["taxes"] + 1)
        feat["tax_rate"] = df["taxes"] / (df["totalPrice"] + 1)
        feat["log_price"] = np.log1p(df["totalPrice"])

        # æ—¶é•¿ç‰¹å¾
        print("   åˆ›å»ºæ—¶é•¿ç‰¹å¾...")
        df["total_duration"] = df["legs0_duration"].fillna(0) + df["legs1_duration"].fillna(0)
        feat["duration_ratio"] = np.where(
            df["legs1_duration"].fillna(0) > 0,
            df["legs0_duration"] / (df["legs1_duration"] + 1),
            1.0,
        )

        # èˆªæ®µæ•°é‡ç‰¹å¾
        print("   åˆ›å»ºèˆªæ®µç‰¹å¾...")
        for leg in (0, 1):
            seg_count = 0
            for seg in range(4):
                col = f"legs{leg}_segments{seg}_duration"
                if col in df.columns:
                    seg_count += df[col].notna().astype(int)
                else:
                    break
            feat[f"n_segments_leg{leg}"] = seg_count
        
        feat["total_segments"] = feat["n_segments_leg0"] + feat["n_segments_leg1"]

        # è¡Œç¨‹ç±»å‹æ£€æµ‹
        feat["is_one_way"] = (
            df["legs1_duration"].isna() | 
            (df["legs1_duration"] == 0) |
            df["legs1_segments0_departureFrom_airport_iata"].isna()
        ).astype(int)
        
        feat["has_return"] = (1 - feat["is_one_way"]).astype(int)

        # æ’åç‰¹å¾
        print("   åˆ›å»ºæ’åç‰¹å¾...")
        if len(df) > 0 and 'ranker_id' in df.columns:
            try:
                grp = df.groupby("ranker_id")
                feat["price_rank"] = grp["totalPrice"].rank()
                feat["price_pct_rank"] = grp["totalPrice"].rank(pct=True)
                feat["duration_rank"] = grp["total_duration"].rank()
                feat["is_cheapest"] = (grp["totalPrice"].transform("min") == df["totalPrice"]).astype(int)
                feat["is_most_expensive"] = (grp["totalPrice"].transform("max") == df["totalPrice"]).astype(int)
                feat["price_from_median"] = grp["totalPrice"].transform(
                    lambda x: (x - x.median()) / (x.std() + 1)
                )
            except Exception as e:
                print(f"   è­¦å‘Š: æ’åç‰¹å¾åˆ›å»ºå¤±è´¥: {e}")
                # åˆ›å»ºé»˜è®¤å€¼
                feat["price_rank"] = 1
                feat["price_pct_rank"] = 0.5
                feat["duration_rank"] = 1
                feat["is_cheapest"] = 0
                feat["is_most_expensive"] = 0
                feat["price_from_median"] = 0

        # å¸¸æ—…å®¢ç‰¹å¾
        print("   åˆ›å»ºå¸¸æ—…å®¢ç‰¹å¾...")
        ff = df["frequentFlyer"].fillna("").astype(str)
        feat["n_ff_programs"] = ff.str.count("/") + (ff != "")
        
        carrier_cols = ["legs0_segments0_marketingCarrier_code", "legs1_segments0_marketingCarrier_code"]
        present_airlines = set()
        for col in carrier_cols:
            if col in df.columns:
                present_airlines.update(df[col].dropna().unique())
        
        for al in ["SU", "S7", "U6", "TK"]:
            if al in present_airlines:
                feat[f"ff_{al}"] = ff.str.contains(rf"\b{al}\b").astype(int)
        
        feat["ff_matches_carrier"] = 0
        for al in ["SU", "S7", "U6", "TK"]:
            if f"ff_{al}" in feat and "legs0_segments0_marketingCarrier_code" in df.columns:
                feat["ff_matches_carrier"] |= (
                    (feat.get(f"ff_{al}", 0) == 1) & 
                    (df["legs0_segments0_marketingCarrier_code"] == al)
                ).astype(int)

        # äºŒå…ƒæ ‡å¿—
        feat["is_vip_freq"] = ((df["isVip"] == 1) | (feat["n_ff_programs"] > 0)).astype(int)
        feat["has_corporate_tariff"] = (~df["corporateTariffCode"].isna()).astype(int)

        # è¡Œæå’Œè´¹ç”¨
        print("   åˆ›å»ºè¡Œæå’Œè´¹ç”¨ç‰¹å¾...")
        feat["baggage_total"] = (
            df["legs0_segments0_baggageAllowance_quantity"].fillna(0)
            + df["legs1_segments0_baggageAllowance_quantity"].fillna(0)
        )
        feat["has_baggage"] = (feat["baggage_total"] > 0).astype(int)
        feat["total_fees"] = (
            df["miniRules0_monetaryAmount"].fillna(0) + df["miniRules1_monetaryAmount"].fillna(0)
        )
        feat["has_fees"] = (feat["total_fees"] > 0).astype(int)
        feat["fee_rate"] = feat["total_fees"] / (df["totalPrice"] + 1)

        # æ—¶é—´ç‰¹å¾
        print("   åˆ›å»ºæ—¶é—´ç‰¹å¾...")
        for col in ("legs0_departureAt", "legs0_arrivalAt", "legs1_departureAt", "legs1_arrivalAt"):
            if col in df.columns:
                dt = pd.to_datetime(df[col], errors="coerce")
                feat[f"{col}_hour"] = dt.dt.hour.fillna(12)
                feat[f"{col}_weekday"] = dt.dt.weekday.fillna(0)
                h = dt.dt.hour.fillna(12)
                feat[f"{col}_business_time"] = (((6 <= h) & (h <= 9)) | ((17 <= h) & (h <= 20))).astype(int)

        # ç›´é£æ£€æµ‹
        print("   åˆ›å»ºç›´é£èˆªç­ç‰¹å¾...")
        feat["is_direct_leg0"] = (feat["n_segments_leg0"] == 1).astype(int)
        feat["is_direct_leg1"] = np.where(
            feat["is_one_way"] == 1,
            0,
            (feat["n_segments_leg1"] == 1).astype(int)
        )
        feat["both_direct"] = (feat["is_direct_leg0"] & feat["is_direct_leg1"]).astype(int)

        # æœ€ä¾¿å®œç›´é£
        if len(df) > 0 and 'ranker_id' in df.columns:
            try:
                df["_is_direct"] = feat["is_direct_leg0"] == 1
                direct_groups = df[df["_is_direct"]].groupby("ranker_id")["totalPrice"]
                if len(direct_groups) > 0:
                    direct_min_price = direct_groups.min()
                    feat["is_direct_cheapest"] = (
                        df["_is_direct"] & 
                        (df["totalPrice"] == df["ranker_id"].map(direct_min_price))
                    ).astype(int)
                else:
                    feat["is_direct_cheapest"] = 0
                df.drop(columns="_is_direct", inplace=True)
            except Exception as e:
                print(f"   è­¦å‘Š: ç›´é£æœ€ä¾¿å®œç‰¹å¾åˆ›å»ºå¤±è´¥: {e}")
                feat["is_direct_cheapest"] = 0

        # å…¶ä»–ç‰¹å¾
        print("   åˆ›å»ºå…¶ä»–ç‰¹å¾...")
        feat["has_access_tp"] = (df["pricingInfo_isAccessTP"] == 1).astype(int)
        
        if len(df) > 0 and 'ranker_id' in df.columns:
            try:
                feat["group_size"] = df.groupby("ranker_id")["Id"].transform("count")
                feat["group_size_log"] = np.log1p(feat["group_size"])
            except Exception as e:
                print(f"   è­¦å‘Š: ç»„å¤§å°ç‰¹å¾åˆ›å»ºå¤±è´¥: {e}")
                feat["group_size"] = 1
                feat["group_size_log"] = 0
        else:
            feat["group_size"] = 1
            feat["group_size_log"] = 0
        
        if "legs0_segments0_marketingCarrier_code" in df.columns:
            feat["is_major_carrier"] = df["legs0_segments0_marketingCarrier_code"].isin(["SU", "S7", "U6"]).astype(int)
        else:
            feat["is_major_carrier"] = 0
        
        popular_routes = {"MOWLED/LEDMOW", "LEDMOW/MOWLED", "MOWLED", "LEDMOW", "MOWAER/AERMOW"}
        feat["is_popular_route"] = df["searchRoute"].isin(popular_routes).astype(int)
        
        feat["avg_cabin_class"] = df[["legs0_segments0_cabinClass", "legs1_segments0_cabinClass"]].mean(axis=1)
        feat["cabin_class_diff"] = (
            df["legs0_segments0_cabinClass"].fillna(0) - df["legs1_segments0_cabinClass"].fillna(0)
        )

        # åˆå¹¶æ–°ç‰¹å¾
        df = pd.concat([df, pd.DataFrame(feat, index=df.index)], axis=1)

        # å¤„ç†ç¼ºå¤±å€¼
        for col in df.select_dtypes(include="number").columns:
            df[col] = df[col].fillna(0)
        for col in df.select_dtypes(include="object").columns:
            df[col] = df[col].fillna("missing")

        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå…±åˆ›å»º {len(feat)} ä¸ªæ–°ç‰¹å¾")
        return df

# %% [markdown]
# ## 8. ç‰¹å¾é€‰æ‹©åŠŸèƒ½

# %%
    def select_features(self, train_df: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """
        é€‰æ‹©ç”¨äºæ¨¡å‹è®­ç»ƒçš„ç‰¹å¾
        
        Args:
            train_df: è®­ç»ƒæ•°æ®
            
        Returns:
            Tuple[List[str], List[str]]: æ‰€æœ‰ç‰¹å¾åˆ—è¡¨å’Œåˆ†ç±»ç‰¹å¾åˆ—è¡¨
        """
        print("å¼€å§‹ç‰¹å¾é€‰æ‹©...")
        
        # åˆ†ç±»ç‰¹å¾
        cat_features = [
            'nationality', 'searchRoute', 'corporateTariffCode',
            # Leg 0 segments 0-1
            'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',
            'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',
            'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',
            'legs0_segments0_flightNumber',
            'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',
            'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',
            'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',
            'legs0_segments1_flightNumber',
            # Leg 1 segments 0-1
            'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',
            'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',
            'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',
            'legs1_segments0_flightNumber',
            'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',
            'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',
            'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',
            'legs1_segments1_flightNumber'
        ]

        # éœ€è¦æ’é™¤çš„åˆ—ï¼ˆæ— ä¿¡æ¯æˆ–æœ‰é—®é¢˜çš„åˆ—ï¼‰
        exclude_cols = [
            'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',
            'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',
            'miniRules0_percentage', 'miniRules1_percentage',  # >90% ç¼ºå¤±
            'frequentFlyer',  # å·²å¤„ç†
            # æ’é™¤å¸¸é‡æˆ–æ¥è¿‘å¸¸é‡çš„åˆ—
            'bySelf', 'pricingInfo_passengerCount',
            # æ’é™¤è¡Œæé‡é‡è®¡é‡ç±»å‹åˆ—ï¼ˆå¯èƒ½æ˜¯å¸¸é‡ï¼‰
            'legs0_segments0_baggageAllowance_weightMeasurementType',
            'legs0_segments1_baggageAllowance_weightMeasurementType',
            'legs1_segments0_baggageAllowance_weightMeasurementType',
            'legs1_segments1_baggageAllowance_weightMeasurementType',
            # æ’é™¤æ•°æ®ä¸­ä¸å­˜åœ¨çš„èˆªç©ºå…¬å¸çš„å¸¸æ—…å®¢ç‰¹å¾
            'ff_DP', 'ff_UT', 'ff_EK', 'ff_N4', 'ff_5N', 'ff_LH'
        ]

        # æ’é™¤segment 2-3åˆ—ï¼ˆ>98%ç¼ºå¤±ï¼‰
        for leg in [0, 1]:
            for seg in [2, 3]:
                for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',
                              'baggageAllowance_quantity', 'baggageAllowance_weightMeasurementType',
                              'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',
                              'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:
                    exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')

        feature_cols = [col for col in train_df.columns if col not in exclude_cols]
        cat_features_final = [col for col in cat_features if col in feature_cols]

        print(f"ç‰¹å¾é€‰æ‹©å®Œæˆ")
        print(f"   - æ€»ç‰¹å¾æ•°: {len(feature_cols)}")
        print(f"   - åˆ†ç±»ç‰¹å¾æ•°: {len(cat_features_final)}")
        
        self.feature_cols = feature_cols
        self.cat_features = cat_features_final
        
        return feature_cols, cat_features_final

# %% [markdown]
# ## 9. XGBoostæ•°æ®é¢„å¤„ç†

# %%
    def prepare_xgb_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        ä¸ºXGBoostå‡†å¤‡æ•°æ®ï¼ˆæ ‡ç­¾ç¼–ç åˆ†ç±»ç‰¹å¾ï¼‰
        
        Args:
            train_df: è®­ç»ƒæ•°æ®
            test_df: æµ‹è¯•æ•°æ®
            
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: å¤„ç†åçš„è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®
        """
        print("ğŸ”„ ä¸ºXGBoostå‡†å¤‡æ•°æ®...")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X_train = train_df[self.feature_cols].copy()
        y_train = train_df['selected']
        groups_train = train_df['ranker_id']
        X_test = test_df[self.feature_cols].copy()
        groups_test = test_df['ranker_id']

        # è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²ï¼ˆæŒ‰ç»„åˆ†å‰²ï¼‰
        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=self.random_state)
        train_idx, val_idx = next(gss.split(X_train, y_train, groups_train))

        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        groups_tr, groups_val = groups_train.iloc[train_idx], groups_train.iloc[val_idx]

        # æ ‡ç­¾ç¼–ç åˆ†ç±»ç‰¹å¾
        print("   ğŸ·ï¸ å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ ‡ç­¾ç¼–ç ...")
        for col in self.cat_features:
            if col in X_tr.columns:
                # ä»æ‰€æœ‰æ•°æ®åˆ›å»ºæ˜ å°„
                unique_vals = pd.concat([X_tr[col], X_val[col], X_test[col]]).unique()
                mapping = {val: idx for idx, val in enumerate(unique_vals)}
                
                X_tr[col] = X_tr[col].map(mapping).fillna(-1).astype(int)
                X_val[col] = X_val[col].map(mapping).fillna(-1).astype(int)
                X_test[col] = X_test[col].map(mapping).fillna(-1).astype(int)

        # åˆ›å»ºæ•°æ®åˆ†å‰²ä¿¡æ¯
        train_split = pd.concat([
            X_tr,
            pd.DataFrame({
                'selected': y_tr,
                'ranker_id': groups_tr
            }, index=X_tr.index)
        ], axis=1)
        
        val_split = pd.concat([
            X_val,
            pd.DataFrame({
                'selected': y_val,
                'ranker_id': groups_val
            }, index=X_val.index)
        ], axis=1)
        
        test_split = pd.concat([
            X_test,
            pd.DataFrame({
                'ranker_id': groups_test
            }, index=X_test.index)
        ], axis=1)

        print(f"æ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"   - è®­ç»ƒé›†: {len(train_split):,} è¡Œ")
        print(f"   - éªŒè¯é›†: {len(val_split):,} è¡Œ")
        print(f"   - æµ‹è¯•é›†: {len(test_split):,} è¡Œ")

        return train_split, val_split, test_split

# %% [markdown]
# ## 10. æ¨¡å‹è®­ç»ƒåŠŸèƒ½

# %%
    def get_memory_usage(self) -> str:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        return f"{memory_mb:.1f} MB"
    
    def optimize_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¼˜åŒ–DataFrameçš„å†…å­˜ä½¿ç”¨
        
        Args:
            df: è¾“å…¥DataFrame
            
        Returns:
            å†…å­˜ä¼˜åŒ–åçš„DataFrame
        """
        print(f"å†…å­˜ä¼˜åŒ–å‰: {self.get_memory_usage()}")
        
        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        # ä¼˜åŒ–æ•°å€¼åˆ—
        for col in df.select_dtypes(include=['int64']).columns:
            col_max = df[col].max()
            col_min = df[col].min()
            
            if col_min >= 0:
                if col_max < 255:
                    df[col] = df[col].astype('uint8')
                elif col_max < 65535:
                    df[col] = df[col].astype('uint16')
                elif col_max < 4294967295:
                    df[col] = df[col].astype('uint32')
            else:
                if col_min > -128 and col_max < 127:
                    df[col] = df[col].astype('int8')
                elif col_min > -32768 and col_max < 32767:
                    df[col] = df[col].astype('int16')
                elif col_min > -2147483648 and col_max < 2147483647:
                    df[col] = df[col].astype('int32')
        
        # ä¼˜åŒ–æµ®ç‚¹æ•°åˆ—
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
        
        # ä¼˜åŒ–å­—ç¬¦ä¸²åˆ—
        for col in df.select_dtypes(include=['object']).columns:
            num_unique_values = len(df[col].unique())
            num_total_values = len(df[col])
            if num_unique_values / num_total_values < 0.5:
                df[col] = df[col].astype('category')
        
        optimized_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        print(f"å†…å­˜ä¼˜åŒ–: {original_memory:.1f} MB â†’ {optimized_memory:.1f} MB "
              f"(èŠ‚çœ {(original_memory - optimized_memory) / original_memory * 100:.1f}%)")
        print(f"å†…å­˜ä¼˜åŒ–å: {self.get_memory_usage()}")
        
        return df

    def train_model(self, train_split: pd.DataFrame, val_split: pd.DataFrame) -> xgb.Booster:
        """
        è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆæ”¯æŒåˆ†å—è®­ç»ƒï¼‰
        
        Args:
            train_split: è®­ç»ƒæ•°æ®
            val_split: éªŒè¯æ•°æ®
            
        Returns:
            è®­ç»ƒå¥½çš„XGBoostæ¨¡å‹
        """
        print("å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        print(f"è®­ç»ƒæ•°æ®å¤§å°: {len(train_split):,} è¡Œ")
        print(f"éªŒè¯æ•°æ®å¤§å°: {len(val_split):,} è¡Œ")
        
        # å†…å­˜ä¼˜åŒ–
        if len(train_split) > self.chunk_size:
            print("æ‰§è¡Œå†…å­˜ä¼˜åŒ–...")
            train_split = self.optimize_memory(train_split)
            val_split = self.optimize_memory(val_split)
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X_train = train_split[self.feature_cols]
        y_train = train_split['selected']
        groups_train = train_split['ranker_id']
        
        X_val = val_split[self.feature_cols]
        y_val = val_split['selected']
        groups_val = val_split['ranker_id']
        
        # åˆ›å»ºç»„å¤§å°
        group_sizes_train = groups_train.value_counts().sort_index().values
        group_sizes_val = groups_val.value_counts().sort_index().values
        
        # æ£€æŸ¥æ•°æ®å¤§å°ï¼Œå†³å®šæ˜¯å¦åˆ†å—è®­ç»ƒ
        if len(train_split) > self.chunk_size * 5:
            print(f"æ•°æ®é‡å¾ˆå¤§ ({len(train_split):,} è¡Œ)ï¼Œè€ƒè™‘ä½¿ç”¨å¢é‡è®­ç»ƒ...")
            return self._train_model_incremental(X_train, y_train, group_sizes_train, 
                                                X_val, y_val, group_sizes_val)
        else:
            return self._train_model_standard(X_train, y_train, group_sizes_train,
                                            X_val, y_val, group_sizes_val)
    
    def _train_model_standard(self, X_train, y_train, group_sizes_train,
                             X_val, y_val, group_sizes_val) -> xgb.Booster:
        """æ ‡å‡†XGBoostè®­ç»ƒ"""
        print("ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼...")
        
        # åˆ›å»ºDMatrix
        dtrain = xgb.DMatrix(X_train, label=y_train, group=group_sizes_train)
        dval = xgb.DMatrix(X_val, label=y_val, group=group_sizes_val)
        
        # XGBoostå‚æ•°
        params = {
            'objective': 'rank:pairwise',
            'eval_metric': 'ndcg@3',
            'max_depth': 6,
            'min_child_weight': 10,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'lambda': 10.0,
            'learning_rate': 0.1,
            'seed': self.random_state,
            'n_jobs': -1
        }
        
        # è®­ç»ƒæ¨¡å‹
        model = xgb.train(
            params,
            dtrain,
            num_boost_round=1000,
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=50,
            verbose_eval=100
        )
        
        return model
    
    def _train_model_incremental(self, X_train, y_train, group_sizes_train,
                                X_val, y_val, group_sizes_val) -> xgb.Booster:
        """å¢é‡XGBoostè®­ç»ƒï¼ˆå¯¹äºå¤§æ•°æ®é›†ï¼‰"""
        print("ä½¿ç”¨å¢é‡è®­ç»ƒæ¨¡å¼...")
        print("æ³¨æ„: å½“å‰XGBoostç‰ˆæœ¬å¯èƒ½ä¸å®Œå…¨æ”¯æŒå¢é‡å­¦ä¹ ")
        
        # å¯¹äºéå¸¸å¤§çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é‡‡æ ·ç­–ç•¥
        sample_size = min(len(X_train), self.chunk_size * 3)
        if sample_size < len(X_train):
            print(f"æ•°æ®é‡è¿‡å¤§ï¼Œé‡‡æ ·åˆ° {sample_size:,} è¡Œè¿›è¡Œè®­ç»ƒ")
            sample_idx = np.random.choice(len(X_train), sample_size, replace=False)
            
            X_train_sampled = X_train.iloc[sample_idx]
            y_train_sampled = y_train.iloc[sample_idx]
            
            # é‡æ–°è®¡ç®—ç»„å¤§å°
            groups_sampled = X_train_sampled.index.map(
                lambda x: X_train.iloc[x:x+1]['ranker_id'].iloc[0] if x < len(X_train) else 'unknown'
            )
            group_sizes_sampled = pd.Series(groups_sampled).value_counts().values
            
            return self._train_model_standard(X_train_sampled, y_train_sampled, group_sizes_sampled,
                                            X_val, y_val, group_sizes_val)
        else:
            return self._train_model_standard(X_train, y_train, group_sizes_train,
                                            X_val, y_val, group_sizes_val)

# %% [markdown]
# ## 11. æ¨¡å‹è¯„ä¼°åŠŸèƒ½

# %%
    def sigmoid(self, x: np.ndarray, scale: float = 10.0) -> np.ndarray:
        """å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆä½¿ç”¨sigmoidå‡½æ•°ï¼‰"""
        return 1 / (1 + np.exp(-x / scale))

    def calculate_hitrate_at_k(self, df: pd.DataFrame, k: int = 3) -> float:
        """è®¡ç®—HitRate@kï¼ˆé’ˆå¯¹è¶…è¿‡10ä¸ªé€‰é¡¹çš„ç»„ï¼‰"""
        hits = []
        for ranker_id, group in df.groupby('ranker_id'):
            if len(group) > 10:
                top_k = group.nlargest(k, 'pred')
                hit = (top_k['selected'] == 1).any()
                hits.append(hit)
        return np.mean(hits) if hits else 0.0

    def evaluate_model(self, y_true: np.ndarray, y_pred: np.ndarray, groups: pd.Series, model_name: str = "æ¨¡å‹") -> Tuple[pd.DataFrame, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹åˆ†æ•°
            groups: åˆ†ç»„ä¿¡æ¯
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Tuple[pd.DataFrame, float]: è¯„ä¼°ç»“æœDataFrameå’ŒHitRate@3
        """
        print(f"è¯„ä¼°{model_name}æ€§èƒ½...")
        
        df = pd.DataFrame({
            'ranker_id': groups,
            'pred': y_pred,
            'selected': y_true
        })
        
        # è·å–æ¯ç»„çš„æœ€é«˜é¢„æµ‹
        top_preds = df.loc[df.groupby('ranker_id')['pred'].idxmax()]
        top_preds['prob'] = self.sigmoid(top_preds['pred'])
        
        # è®¡ç®—æŒ‡æ ‡
        logloss = log_loss(top_preds['selected'], top_preds['prob'])
        hitrate_at_3 = self.calculate_hitrate_at_k(df, k=3)
        accuracy = (top_preds['selected'] == 1).mean()
        
        print(f"{model_name} éªŒè¯æŒ‡æ ‡:")
        print(f"   HitRate@3 (ç»„å¤§å°>10): {hitrate_at_3:.4f}")
        print(f"   LogLoss:                {logloss:.4f}")
        print(f"   Top-1 å‡†ç¡®ç‡:           {accuracy:.4f}")
        
        return df, hitrate_at_3

# %% [markdown]
# ## 12. ç‰¹å¾é‡è¦æ€§åˆ†æ

# %%
    def get_feature_importance(self) -> pd.DataFrame:
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        Returns:
            pd.DataFrame: ç‰¹å¾é‡è¦æ€§æ’åº
        """
        if not hasattr(self, 'model') or self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•è·å–ç‰¹å¾é‡è¦æ€§")
            return pd.DataFrame()
        
        print("åˆ†æç‰¹å¾é‡è¦æ€§...")
        
        # è·å–XGBoostç‰¹å¾é‡è¦æ€§
        importance = self.model.get_score(importance_type='gain')
        
        # è½¬æ¢ä¸ºDataFrame
        importance_df = pd.DataFrame([
            {'feature': k, 'importance': v} 
            for k, v in importance.items()
        ]).sort_values('importance', ascending=False)
        
        print(f"ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆï¼Œå…± {len(importance_df)} ä¸ªç‰¹å¾")
        print("Top 20 é‡è¦ç‰¹å¾:")
        print(importance_df.head(20).to_string(index=False))
        
        return importance_df

# %% [markdown]
# ## 13. ç”Ÿæˆé¢„æµ‹ç»“æœ

# %%
    def generate_predictions(self, test_split: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹ç»“æœ
        
        Args:
            test_split: æµ‹è¯•æ•°æ®
            
        Returns:
            pd.DataFrame: æäº¤æ ¼å¼çš„é¢„æµ‹ç»“æœ
        """
        if not hasattr(self, 'model') or self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹")
            return pd.DataFrame()
        
        print("ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test = test_split[self.feature_cols]
        groups_test = test_split['ranker_id']
        
        # åˆ›å»ºç»„å¤§å°
        group_sizes_test = test_split.groupby('ranker_id').size().values
        dtest = xgb.DMatrix(X_test, group=group_sizes_test)
        
        # ç”Ÿæˆé¢„æµ‹
        test_preds = self.model.predict(dtest)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'Id': test_split.index,  # ä½¿ç”¨åŸå§‹çš„Id
            'ranker_id': groups_test,
            'pred_score': test_preds
        })
        
        # æ ¹æ®ranker_idåˆ†ç»„ï¼ŒæŒ‰é¢„æµ‹åˆ†æ•°æ’å
        submission['selected'] = submission.groupby('ranker_id')['pred_score'].rank(
            ascending=False, method='first'
        ).astype(int)
        
        # åªä¿ç•™éœ€è¦çš„åˆ—
        final_submission = submission[['Id', 'ranker_id', 'selected']].copy()
        
        print(f"é¢„æµ‹ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {final_submission.shape}")
        
        return final_submission

# %% [markdown]
# ## 14. å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹

# %%
    def run_full_pipeline(self, train_path: str = 'train.parquet', test_path: str = 'test.parquet') -> pd.DataFrame:
        """
        è¿è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹
        
        Args:
            train_path: è®­ç»ƒæ•°æ®è·¯å¾„
            test_path: æµ‹è¯•æ•°æ®è·¯å¾„
            
        Returns:
            pd.DataFrame: æœ€ç»ˆçš„é¢„æµ‹ç»“æœ
        """
        print("å¼€å§‹å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹...")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        train_df, test_df = self.load_data(train_path, test_path)
        if train_df.empty or test_df.empty:
            print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return pd.DataFrame()
        
        # 2. æ•°æ®é‡‡æ ·
        train_df = self.sample_data(train_df)
        
        # 3. ç‰¹å¾å·¥ç¨‹
        print("\n" + "="*60)
        train_df = self.create_features(train_df)
        test_df = self.create_features(test_df)
        
        # 4. ç‰¹å¾é€‰æ‹©
        print("\n" + "="*60)
        self.select_features(train_df)
        
        # 5. æ•°æ®é¢„å¤„ç†
        print("\n" + "="*60)
        train_split, val_split, test_split = self.prepare_xgb_data(train_df, test_df)
        
        # 6. æ¨¡å‹è®­ç»ƒ
        print("\n" + "="*60)
        self.train_model(train_split, val_split)
        
        # 7. æ¨¡å‹è¯„ä¼°
        print("\n" + "="*60)
        val_preds = self.model.predict(xgb.DMatrix(
            val_split[self.feature_cols],
            group=val_split.groupby('ranker_id').size().values
        ))
        self.evaluate_model(val_split['selected'].values, val_preds, val_split['ranker_id'], "XGBoost")
        
        # 8. ç‰¹å¾é‡è¦æ€§
        print("\n" + "="*60)
        self.get_feature_importance()
        
        # 9. ç”Ÿæˆé¢„æµ‹
        print("\n" + "="*60)
        submission = self.generate_predictions(test_split)
        
        # 10. ä¿å­˜ç»“æœ
        if not submission.empty:
            submission.to_csv('submission.csv', index=False)
            print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° submission.csv")
        
        print("\n" + "="*60)
        print("å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹å®Œæˆï¼")
        
        return submission

# %% [markdown]
# ## 15. å¿«é€Ÿè¿è¡Œå‡½æ•°

# %%
def quick_run_large_dataset(train_path: str, test_path: str, output_path: str = 'submission.csv', 
                           sample_frac: float = 0.3, chunk_size: int = 50000, random_state: int = 42):
    """
    å¤§æ•°æ®é›†å¿«é€Ÿè¿è¡Œå‡½æ•°ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        train_path: è®­ç»ƒæ•°æ®è·¯å¾„
        test_path: æµ‹è¯•æ•°æ®è·¯å¾„  
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        sample_frac: é‡‡æ ·æ¯”ä¾‹ï¼ˆå»ºè®®0.1-0.5ï¼‰
        chunk_size: åˆ†å—å¤§å°
        random_state: éšæœºç§å­
        
    Returns:
        pd.DataFrame: æäº¤ç»“æœ
    """
    print("=== å¤§æ•°æ®é›†ä¼˜åŒ–æ¨¡å¼ ===")
    print(f"å†…å­˜ä¼˜åŒ–æ¨¡å¼")
    print(f"æ–‡ä»¶é…ç½®:")
    print(f"   è®­ç»ƒæ•°æ®: {train_path}")
    print(f"   æµ‹è¯•æ•°æ®: {test_path}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {sample_frac}")
    print(f"   åˆ†å—å¤§å°: {chunk_size:,}")
    print()
    
    try:
        # åˆ›å»ºæ¨¡å‹ï¼ˆå¯ç”¨åˆ†å—åŠ è½½å’Œå†…å­˜ä¼˜åŒ–ï¼‰
        model = FlightRankingModel(
            train_sample_frac=sample_frac,
            random_state=random_state,
            chunk_size=chunk_size,
            use_chunked_loading=True  # å¯ç”¨åˆ†å—åŠ è½½
        )
        
        # è¿è¡Œæµç¨‹
        print("å¼€å§‹å¤§æ•°æ®é›†å¤„ç†æµç¨‹...")
        submission = model.run_full_pipeline(train_path, test_path)
        
        if not submission.empty:
            # ä¿å­˜ç»“æœ
            submission[['Id', 'selected']].to_csv(output_path, index=False)
            print(f"\nè¿è¡Œå®Œæˆï¼")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            print(f"ç”Ÿæˆäº† {len(submission):,} è¡Œé¢„æµ‹ç»“æœ")
            return submission
        else:
            print("è¿è¡Œå¤±è´¥")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def quick_run(train_path: str, test_path: str, output_path: str = 'submission.csv', 
              sample_frac: float = 0.5, random_state: int = 42):
    """
    å¿«é€Ÿè¿è¡Œå‡½æ•°
    
    Args:
        train_path: è®­ç»ƒæ•°æ®è·¯å¾„
        test_path: æµ‹è¯•æ•°æ®è·¯å¾„  
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        sample_frac: é‡‡æ ·æ¯”ä¾‹
        random_state: éšæœºç§å­
        
    Returns:
        pd.DataFrame: æäº¤ç»“æœ
    """
    print("å¿«é€Ÿè¿è¡Œæ¨¡å¼")
    print(f"æ–‡ä»¶é…ç½®:")
    print(f"   è®­ç»ƒæ•°æ®: {train_path}")
    print(f"   æµ‹è¯•æ•°æ®: {test_path}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {sample_frac}")
    print()
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = FlightRankingModel(
            train_sample_frac=sample_frac,
            random_state=random_state
        )
        
        # è¿è¡Œæµç¨‹
        submission = model.run_full_pipeline(train_path, test_path)
        
        if not submission.empty:
            # ä¿å­˜ç»“æœ
            submission[['Id', 'selected']].to_csv(output_path, index=False)
            print(f"\nè¿è¡Œå®Œæˆï¼")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            print(f"ç”Ÿæˆäº† {len(submission):,} è¡Œé¢„æµ‹ç»“æœ")
            return submission
        else:
            print("è¿è¡Œå¤±è´¥")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")
        return pd.DataFrame()

# %% [markdown]
# ## 16. ä¸»å‡½æ•°å’Œæ‰§è¡Œ

# %%
def main():
    """ä¸»å‡½æ•°"""
    
    # ===== æ–‡ä»¶è·¯å¾„é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œä¿®æ”¹æ‚¨çš„æ–‡ä»¶è·¯å¾„ =====
    TRAIN_FILE_PATH = 'train.parquet'        # ä¿®æ”¹è¿™é‡Œè®¾ç½®è®­ç»ƒæ–‡ä»¶è·¯å¾„
    TEST_FILE_PATH = 'test.parquet'          # ä¿®æ”¹è¿™é‡Œè®¾ç½®æµ‹è¯•æ–‡ä»¶è·¯å¾„
    OUTPUT_FILE_PATH = 'submission.csv'      # ä¿®æ”¹è¿™é‡Œè®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    # æ€§èƒ½é…ç½®
    USE_LARGE_DATASET_MODE = True           # æ˜¯å¦ä½¿ç”¨å¤§æ•°æ®é›†ä¼˜åŒ–æ¨¡å¼
    CHUNK_SIZE = 50000                      # åˆ†å—å¤§å°
    SAMPLE_FRAC = 0.3                       # é‡‡æ ·æ¯”ä¾‹ï¼ˆå¤§æ•°æ®é›†æ¨¡å¼ä¸‹å»ºè®®0.1-0.5ï¼‰
    # ==========================================================
    
    try:
        print("=== AeroClub RecSys 2025 - æ”¹è¿›çš„XGBoostæ’åºæ¨¡å‹ ===")
        print(f"ä½¿ç”¨çš„æ–‡ä»¶è·¯å¾„:")
        print(f"   è®­ç»ƒæ•°æ®: {TRAIN_FILE_PATH}")
        print(f"   æµ‹è¯•æ•°æ®: {TEST_FILE_PATH}")
        print(f"   è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE_PATH}")
        
        if USE_LARGE_DATASET_MODE:
            print(f"   è¿è¡Œæ¨¡å¼: å¤§æ•°æ®é›†ä¼˜åŒ–æ¨¡å¼")
            print(f"   åˆ†å—å¤§å°: {CHUNK_SIZE:,}")
            print(f"   é‡‡æ ·æ¯”ä¾‹: {SAMPLE_FRAC}")
        else:
            print(f"   è¿è¡Œæ¨¡å¼: æ ‡å‡†æ¨¡å¼")
            print(f"   é‡‡æ ·æ¯”ä¾‹: {TRAIN_SAMPLE_FRAC}")
        print()
        
        if USE_LARGE_DATASET_MODE:
            # ä½¿ç”¨å¤§æ•°æ®é›†ä¼˜åŒ–æ¨¡å¼
            submission = quick_run_large_dataset(
                TRAIN_FILE_PATH, 
                TEST_FILE_PATH, 
                OUTPUT_FILE_PATH,
                sample_frac=SAMPLE_FRAC,
                chunk_size=CHUNK_SIZE,
                random_state=RANDOM_STATE
            )
        else:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = FlightRankingModel(
                train_sample_frac=TRAIN_SAMPLE_FRAC,
                random_state=RANDOM_STATE
            )
            
            # è¿è¡Œå®Œæ•´æµç¨‹
            submission = model.run_full_pipeline(TRAIN_FILE_PATH, TEST_FILE_PATH)
        
        if not submission.empty:
            # ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
            submission[['Id', 'selected']].to_csv(OUTPUT_FILE_PATH, index=False)
            print(f"\nä»»åŠ¡å®Œæˆï¼ç”Ÿæˆäº† {len(submission)} è¡Œé¢„æµ‹ç»“æœ")
            print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE_PATH}")
        else:
            print("\nä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œé”™è¯¯ä¿¡æ¯")
            
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

# %%
if __name__ == "__main__":
    main()

# %% [markdown]
# ## 17. ç»“è¯­
# 
# è¿™ä¸ªæ”¹è¿›çš„æ¨¡å‹åŒ…å«äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
# 
# ### ä¸»è¦æ”¹è¿›ç‚¹ï¼š
# 1. **å®Œæ•´çš„é¢å‘å¯¹è±¡è®¾è®¡** - ä½¿ç”¨ `FlightRankingModel` ç±»å°è£…æ‰€æœ‰åŠŸèƒ½
# 2. **å…¨é¢çš„ç‰¹å¾å·¥ç¨‹** - ä»·æ ¼ã€æ—¶é—´ã€æ’åã€ç”¨æˆ·ç‰¹å¾ç­‰
# 3. **å®Œå–„çš„é”™è¯¯å¤„ç†** - æ–‡ä»¶ä¸å­˜åœ¨ã€æ•°æ®é—®é¢˜ç­‰å¼‚å¸¸å¤„ç†
# 4. **è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š** - æ¯ä¸ªå‡½æ•°å’Œé‡è¦æ­¥éª¤éƒ½æœ‰æ¸…æ™°è¯´æ˜
# 5. **å†…å­˜ä¼˜åŒ–** - æ•°æ®é‡‡æ ·ã€é€‚å½“çš„æ•°æ®ç±»å‹è½¬æ¢
# 6. **æ¨¡å‹è¯„ä¼°** - HitRate@3ã€LogLossã€ç‰¹å¾é‡è¦æ€§åˆ†æ
# 7. **å¤§æ•°æ®é›†ä¼˜åŒ–** - åˆ†å—åŠ è½½å’Œå†…å­˜ä¼˜åŒ–
# 
# ### æ”¯æŒçš„åŠŸèƒ½ï¼š
# - æ•°æ®åŠ è½½å’ŒéªŒè¯
# - æ™ºèƒ½æ•°æ®é‡‡æ ·ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
# - å¤šç§ç±»å‹çš„ç‰¹å¾å·¥ç¨‹
# - XGBoostæ’åºæ¨¡å‹è®­ç»ƒ
# - å…¨é¢çš„æ¨¡å‹è¯„ä¼°
# - é¢„æµ‹ç»“æœç”Ÿæˆå’Œä¿å­˜
# 
# ### ä½¿ç”¨æ–¹æ³•ï¼š
# åªéœ€è¦è¿è¡Œ `main()` å‡½æ•°å³å¯å®Œæˆæ•´ä¸ªæœºå™¨å­¦ä¹ æµç¨‹ï¼
